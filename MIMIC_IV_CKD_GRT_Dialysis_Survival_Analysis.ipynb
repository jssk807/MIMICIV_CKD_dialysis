{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "pSIOtg1o49Vd",
        "aiLpqfOk5PXi",
        "jDo5JIU9qKrQ",
        "V4MKh4X4qBPo",
        "otT16ozmsJh4",
        "XZR6JCKMwBNY",
        "08hNkh2HygYe",
        "MpkUdM3e5qMP",
        "fcd53bc0",
        "59703b63",
        "4efa9c96",
        "ab3e1535",
        "5de94a96",
        "404d300d",
        "7d645c22",
        "6e3ab5e1",
        "2434137d",
        "e37def74",
        "e321d959",
        "ed9c6dad",
        "951d09ba",
        "da758ac0",
        "530ff64f",
        "b515d171",
        "940a2b92",
        "fb7731ea",
        "b0303bfd",
        "fb132eef",
        "b176611e",
        "nKMkOAS9X7dU",
        "46fce516",
        "dc96a015",
        "ecdcfb8d"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load Files"
      ],
      "metadata": {
        "id": "pSIOtg1o49Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load admissions, d_hcpcs, d_icd_diagnoses, d_icd_procedures, diagnoses_icd,\n",
        "# hpcsevents, procedures_icd, and prescriptions tables\n",
        "\n",
        "# Renamed dictionary with prefixes\n",
        "file_columns = {\n",
        "    'hcpcsevents.csv': {'prefix': 'hcpcs_', 'cols': ['subject_id', 'hadm_id', 'hcpcs_cd', 'chartdate', 'short_description']},\n",
        "    'd_hcpcs.csv': {'prefix': 'd_hcpcs_', 'cols': ['code', 'category', 'long_description', 'short_description']},\n",
        "    'diagnoses_icd.csv': {'prefix': 'diag_', 'cols': ['subject_id', 'hadm_id', 'icd_code', 'icd_version']},\n",
        "    'd_icd_diagnoses.csv': {'prefix': 'd_diag_', 'cols': ['icd_code', 'icd_version', 'long_title']},\n",
        "    'procedures_icd.csv': {'prefix': 'proc_', 'cols': ['subject_id', 'hadm_id', 'chartdate', 'icd_code', 'icd_version']},\n",
        "    'd_icd_procedures.csv': {'prefix': 'd_proc_', 'cols': ['icd_code', 'icd_version', 'long_title']},\n",
        "    'admissions.csv': {'prefix': 'admit_', 'cols': ['subject_id', 'hadm_id', 'admittime', 'dischtime']},\n",
        "    'prescriptions.csv': {'prefix': 'presc_', 'cols': ['subject_id', 'hadm_id', 'drug', 'starttime']}\n",
        "}\n",
        "\n",
        "# Define data types for relevant columns in each file\n",
        "file_dtypes = {\n",
        "    'hcpcsevents.csv': {'subject_id': 'int64', 'hadm_id': 'int64', 'hcpcs_cd': 'object', 'short_description': 'object'},\n",
        "    'd_hcpcs.csv': {'code': 'object', 'category': 'object', 'long_description': 'object', 'short_description': 'object'},\n",
        "    'diagnoses_icd.csv': {'subject_id': 'int64', 'hadm_id': 'int64', 'icd_code': 'object', 'icd_version': 'int64'},\n",
        "    'd_icd_diagnoses.csv': {'icd_code': 'object', 'icd_version': 'int64', 'long_title': 'object'},\n",
        "    'procedures_icd.csv': {'subject_id': 'int64', 'hadm_id': 'int64', 'icd_code': 'object', 'icd_version': 'int64'},\n",
        "    'd_icd_procedures.csv': {'icd_code': 'object', 'icd_version': 'int64', 'long_title': 'object'},\n",
        "    'admissions.csv': {'subject_id': 'int64', 'hadm_id': 'int64'},\n",
        "    'prescriptions.csv': {'subject_id': 'int64', 'hadm_id': 'int64', 'drug': 'object'}\n",
        "}\n",
        "\n",
        "# Define date columns for parsing\n",
        "date_columns = {\n",
        "    'hcpcsevents.csv': ['chartdate'],\n",
        "    'procedures_icd.csv': ['chartdate'],\n",
        "    'admissions.csv': ['admittime','dischtime'],\n",
        "    'prescriptions.csv': ['starttime']\n",
        "}"
      ],
      "metadata": {
        "id": "kIka70CEiYgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "dataframe_dict = {}\n",
        "csv_files = os.listdir(folder_path)\n",
        "\n",
        "for file in csv_files:\n",
        "    if file in file_columns and file in file_dtypes:\n",
        "        file_info = file_columns[file]\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "        dtypes_info = file_dtypes[file]\n",
        "        parse_dates_info = date_columns.get(file, [])\n",
        "\n",
        "        try:\n",
        "            if file in ['procedures_icd.csv', 'diagnoses_icd.csv']:\n",
        "                df = pd.read_csv(file_path,\n",
        "                                 usecols=file_info['cols'],\n",
        "                                 low_memory=False,\n",
        "                                 parse_dates=parse_dates_info)\n",
        "\n",
        "                # Handle missing 'subject_id' by removing rows\n",
        "                if 'subject_id' in df.columns:\n",
        "                    df['subject_id'] = pd.to_numeric(df['subject_id'], errors='coerce')\n",
        "                    df = df.dropna(subset=['subject_id']).copy()\n",
        "\n",
        "                    # Convert 'subject_id' to int64 after removing NaNs\n",
        "                    try:\n",
        "                        df['subject_id'] = df['subject_id'].astype('int64')\n",
        "                    except ValueError:\n",
        "                        print(f\"Could not convert 'subject_id' to int64 in {file} after dropping NaNs. Possible non-numeric values remaining.\")\n",
        "\n",
        "            else:\n",
        "                # Read other files with specified dtypes and parse dates\n",
        "                df = pd.read_csv(file_path,\n",
        "                                 usecols=file_info['cols'],\n",
        "                                 dtype=dtypes_info,\n",
        "                                 parse_dates=parse_dates_info)\n",
        "\n",
        "            # Create a dictionary for renaming columns\n",
        "            rename_dict = {col: file_info['prefix'] + col for col in df.columns}\n",
        "\n",
        "            # Rename the columns\n",
        "            df = df.rename(columns=rename_dict)\n",
        "            dataframe_dict[file] = df\n",
        "            print(f\"Successfully loaded and processed {file}\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while processing {file}: {e}\")\n"
      ],
      "metadata": {
        "id": "aC2b4whzipDN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge descriptions files with their counterparts"
      ],
      "metadata": {
        "id": "aiLpqfOk5PXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge diagnoses_icd and d_icd_diagnoses and Filter Out CKD Datapoints"
      ],
      "metadata": {
        "id": "jDo5JIU9qKrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diagnoses_icd_df = dataframe_dict['diagnoses_icd.csv']\n",
        "d_icd_diagnoses_df = dataframe_dict['d_icd_diagnoses.csv']\n",
        "\n",
        "# Merge diagnoses_icd and d_icd_diagnoses to get diagnosis descriptions\n",
        "diagnoses_merged_df = pd.merge(diagnoses_icd_df, d_icd_diagnoses_df, how='left',\n",
        "                               left_on=['diag_icd_code', 'diag_icd_version'],\n",
        "                               right_on=['d_diag_icd_code', 'd_diag_icd_version'])\n",
        "\n",
        "# Drop the redundant columns from the right dataframe after merging\n",
        "diagnoses_merged_df = diagnoses_merged_df.drop(columns=['d_diag_icd_code', 'd_diag_icd_version'])\n",
        "\n",
        "# Rename the remaining columns\n",
        "diagnoses_merged_df = diagnoses_merged_df.rename(columns={'d_diag_long_title': 'diag_long_title'})\n",
        "\n",
        "\n",
        "print(\"Head of the merged diagnoses_merged_df:\")\n",
        "display(diagnoses_merged_df.head())\n",
        "\n",
        "print(\"\\nInfo of the merged diagnoses_merged_df:\")\n",
        "diagnoses_merged_df.info()"
      ],
      "metadata": {
        "id": "13kyrn4lc4hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e733a493"
      },
      "source": [
        "# Filter diagnoses_merged_df to include only rows with specified ICD codes\n",
        "ckd_icd_codes = [\n",
        "    '40300', '40301', '40310', '40311', '40390', '40391', '40400', '40401',\n",
        "    '40402', '40403', '40410', '40411', '40412', '40413', '40490', '40491',\n",
        "    '40492', '40493', '5851', '5852', '5853', '5854', '5855', '5859', 'D631',\n",
        "    'E0822', 'E0922', 'E1022', 'E1122', 'E1322', 'I12', 'I120', 'I129', 'I13',\n",
        "    'I130', 'I131', 'I1310', 'I1311', 'I132', 'N18', 'N181', 'N182', 'N183',\n",
        "    'N1830', 'N1831', 'N1832', 'N184', 'N185', 'N189', 'O102', 'O1021',\n",
        "    'O10211', 'O10212', 'O10213', 'O10219', 'O1022', 'O1023', 'O103',\n",
        "    'O1031', 'O10311', 'O10312', 'O10313', 'O10319', 'O1032', 'O1033'\n",
        "]\n",
        "\n",
        "filtered_diagnoses_merged_df = diagnoses_merged_df[diagnoses_merged_df['diag_icd_code'].isin(ckd_icd_codes)].copy()\n",
        "\n",
        "print(\"Head of filtered_diagnoses_merged_df:\")\n",
        "display(filtered_diagnoses_merged_df.head())\n",
        "\n",
        "print(\"\\nInfo of filtered_diagnoses_merged_df:\")\n",
        "filtered_diagnoses_merged_df.info()\n",
        "\n",
        "del diagnoses_icd_df\n",
        "del d_icd_diagnoses_df\n",
        "del diagnoses_merged_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge hcpcsevents and d_hcpcs"
      ],
      "metadata": {
        "id": "V4MKh4X4qBPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the dataframes\n",
        "hcpcsevents_df = dataframe_dict['hcpcsevents.csv']\n",
        "d_hcpcs_df = dataframe_dict['d_hcpcs.csv']\n",
        "\n",
        "# Merge hcpcsevents and d_hcpcs to get descriptions\n",
        "hcpcsevents_merged_df = pd.merge(hcpcsevents_df, d_hcpcs_df, how='left',\n",
        "                                 left_on='hcpcs_hcpcs_cd',\n",
        "                                 right_on='d_hcpcs_code')\n",
        "\n",
        "# Drop redundant columns and rename for consistency\n",
        "# Drop the redundant code and one of the short descriptions\n",
        "hcpcsevents_merged_df = hcpcsevents_merged_df.drop(columns=['d_hcpcs_code', 'hcpcs_short_description'])\n",
        "\n",
        "# Rename remaining d_hcpcs columns to hcpcs_ prefix\n",
        "hcpcsevents_merged_df = hcpcsevents_merged_df.rename(columns={\n",
        "    'd_hcpcs_category': 'hcpcs_category',\n",
        "    'd_hcpcs_long_description': 'hcpcs_long_description',\n",
        "    'd_hcpcs_short_description': 'hcpcs_short_description',\n",
        "    'hcpcs_hcpcs_cd': 'hcpcs_cd'\n",
        "})\n",
        "\n",
        "\n",
        "# Define the desired column order\n",
        "desired_column_order = [\n",
        "    'hcpcs_subject_id',\n",
        "    'hcpcs_hadm_id',\n",
        "    'hcpcs_chartdate',\n",
        "    'hcpcs_cd',\n",
        "    'hcpcs_category',\n",
        "    'hcpcs_short_description',\n",
        "    'hcpcs_long_description'\n",
        "]\n",
        "\n",
        "# Create a list of existing columns to only select those present in the dataframe\n",
        "existing_columns_in_order = [col for col in desired_column_order if col in hcpcsevents_merged_df.columns]\n",
        "\n",
        "hcpcsevents_merged_df = hcpcsevents_merged_df[existing_columns_in_order]\n",
        "\n",
        "\n",
        "print(\"Head of the merged and cleaned hcpcsevents_merged_df:\")\n",
        "display(hcpcsevents_merged_df.head())\n",
        "\n",
        "print(\"\\nInfo of the merged and cleaned hcpcsevents_merged_df:\")\n",
        "hcpcsevents_merged_df.info()\n",
        "\n",
        "del hcpcsevents_df\n",
        "del d_hcpcs_df"
      ],
      "metadata": {
        "id": "F4ze1HMmpT-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge procedures_icd and d_icd_proceudres"
      ],
      "metadata": {
        "id": "otT16ozmsJh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the dataframes\n",
        "procedures_icd_df = dataframe_dict['procedures_icd.csv']\n",
        "d_icd_procedures_df = dataframe_dict['d_icd_procedures.csv']\n",
        "\n",
        "# Merge procedures_icd and d_icd_procedures to get descriptions\n",
        "procedures_merged_df = pd.merge(procedures_icd_df, d_icd_procedures_df, how='left',\n",
        "                                left_on=['proc_icd_code', 'proc_icd_version'],\n",
        "                                right_on=['d_proc_icd_code', 'd_proc_icd_version'])\n",
        "\n",
        "# Drop redundant columns from the right dataframe after merging\n",
        "procedures_merged_df = procedures_merged_df.drop(columns=['d_proc_icd_code', 'd_proc_icd_version'])\n",
        "\n",
        "# Rename the remaining column from d_icd_procedures_df\n",
        "procedures_merged_df = procedures_merged_df.rename(columns={'d_proc_long_title': 'proc_long_title'})\n",
        "\n",
        "# Define the desired column order\n",
        "desired_column_order = [\n",
        "    'proc_subject_id',\n",
        "    'proc_hadm_id',\n",
        "    'proc_chartdate',\n",
        "    'proc_icd_code',\n",
        "    'proc_icd_version',\n",
        "    'proc_long_title'\n",
        "]\n",
        "\n",
        "# Reindex the dataframe to reorder columns\n",
        "procedures_merged_df = procedures_merged_df[desired_column_order]\n",
        "\n",
        "print(\"Head of the merged procedures_merged_df:\")\n",
        "display(procedures_merged_df.head())\n",
        "\n",
        "print(\"\\nInfo of the merged procedures_merged_df:\")\n",
        "procedures_merged_df.info()\n",
        "\n",
        "del procedures_icd_df\n",
        "del d_icd_procedures_df"
      ],
      "metadata": {
        "id": "_925VAhJsIN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge Files w Sample"
      ],
      "metadata": {
        "id": "XZR6JCKMwBNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the dataframes\n",
        "admissions_df = dataframe_dict['admissions.csv']\n",
        "prescriptions_df = dataframe_dict['prescriptions.csv']\n",
        "hcpcs_events_merged_df_copy = hcpcsevents_merged_df.copy() # Create a copy to work with\n",
        "procedures_merged_df_copy = procedures_merged_df.copy() # Create a copy to work with\n",
        "\n",
        "\n",
        "# Get the unique subject_id and hadm_id from the filtered diagnoses dataframe\n",
        "unique_ids = filtered_diagnoses_merged_df[['diag_subject_id', 'diag_hadm_id']].drop_duplicates()\n",
        "\n",
        "# Filter admissions_df\n",
        "# Rename columns to match for filtering\n",
        "admissions_df_filtered = admissions_df.rename(columns={'admit_subject_id': 'diag_subject_id', 'admit_hadm_id': 'diag_hadm_id'})\n",
        "admissions_df_filtered = pd.merge(unique_ids, admissions_df_filtered, on=['diag_subject_id', 'diag_hadm_id'], how='left')\n",
        "# Rename back to original admissions column names after filtering\n",
        "admissions_df_filtered = admissions_df_filtered.rename(columns={'diag_subject_id': 'admit_subject_id', 'diag_hadm_id': 'admit_hadm_id'})\n",
        "del admissions_df # Delete original dataframe after it's no longer needed\n",
        "\n",
        "\n",
        "# Filter prescriptions_df\n",
        "# Rename columns to match for filtering\n",
        "prescriptions_df_filtered = prescriptions_df.rename(columns={'presc_subject_id': 'diag_subject_id', 'presc_hadm_id': 'diag_hadm_id'})\n",
        "prescriptions_df_filtered = pd.merge(unique_ids, prescriptions_df_filtered, on=['diag_subject_id', 'diag_hadm_id'], how='left')\n",
        "# Rename back to original prescriptions column names after filtering\n",
        "prescriptions_df_filtered = prescriptions_df_filtered.rename(columns={'diag_subject_id': 'presc_subject_id', 'diag_hadm_id': 'presc_hadm_id'})\n",
        "del prescriptions_df # Delete original dataframe after it's no longer needed\n",
        "\n",
        "\n",
        "# Filter hcpcsevents_merged_df\n",
        "hcpcsevents_merged_df_filtered = hcpcs_events_merged_df_copy.rename(columns={'hcpcs_subject_id': 'diag_subject_id', 'hcpcs_hadm_id': 'diag_hadm_id'})\n",
        "hcpcsevents_merged_df_filtered = pd.merge(unique_ids, hcpcsevents_merged_df_filtered, on=['diag_subject_id', 'diag_hadm_id'], how='left')\n",
        "hcpcsevents_merged_df_filtered = hcpcsevents_merged_df_filtered.rename(columns={'diag_subject_id': 'hcpcs_subject_id', 'diag_hadm_id': 'hcpcs_hadm_id'})\n",
        "# del hcpcsevents_merged_df # Removed: do not delete the global variable\n",
        "\n",
        "# Filter procedures_merged_df\n",
        "procedures_merged_df_filtered = procedures_merged_df_copy.rename(columns={'proc_subject_id': 'diag_subject_id', 'proc_hadm_id': 'diag_hadm_id'})\n",
        "procedures_merged_df_filtered = pd.merge(unique_ids, procedures_merged_df_filtered, on=['diag_subject_id', 'diag_hadm_id'], how='left')\n",
        "procedures_merged_df_filtered = procedures_merged_df_filtered.rename(columns={'diag_subject_id': 'proc_subject_id', 'diag_hadm_id': 'proc_hadm_id'})\n",
        "# del procedures_merged_df # Removed: do not delete the global variable"
      ],
      "metadata": {
        "id": "UNjbiN1yBc9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd441c56"
      },
      "source": [
        "# Start with the filtered diagnoses dataframe as the base\n",
        "merged_df = filtered_diagnoses_merged_df\n",
        "\n",
        "# Merge with filtered admissions_df\n",
        "merged_df = pd.merge(merged_df, admissions_df_filtered,\n",
        "                                how='left',\n",
        "                                left_on=['diag_subject_id', 'diag_hadm_id'],\n",
        "                                right_on=['admit_subject_id', 'admit_hadm_id'])\n",
        "\n",
        "# Drop redundant ID columns after merging admissions\n",
        "merged_df = merged_df.drop(columns=['admit_subject_id', 'admit_hadm_id'])\n",
        "# Delete filtered dataframe after merge\n",
        "del admissions_df_filtered\n",
        "\n",
        "# Merge with filtered prescriptions_df\n",
        "merged_df = pd.merge(merged_df, prescriptions_df_filtered,\n",
        "                                how='left',\n",
        "                                left_on=['diag_subject_id', 'diag_hadm_id'],\n",
        "                                right_on=['presc_subject_id', 'presc_hadm_id'])\n",
        "\n",
        "# Drop redundant ID columns after merging prescriptions\n",
        "merged_df = merged_df.drop(columns=['presc_subject_id', 'presc_hadm_id'])\n",
        "# Delete filtered dataframe after merge\n",
        "del prescriptions_df_filtered\n",
        "\n",
        "# Display the head and info of the current merged dataframe\n",
        "print(\"Head of the merged dataframe after adding admissions and prescriptions:\")\n",
        "display(merged_df.head())\n",
        "\n",
        "print(\"\\nInfo of the merged dataframe after adding admissions and prescriptions:\")\n",
        "merged_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78bd59c1"
      },
      "source": [
        "# Merge with filtered hcpcsevents_merged_df\n",
        "merged_df = pd.merge(merged_df, hcpcsevents_merged_df_filtered,\n",
        "                                how='left',\n",
        "                                left_on=['diag_subject_id', 'diag_hadm_id'],\n",
        "                                right_on=['hcpcs_subject_id', 'hcpcs_hadm_id'])\n",
        "\n",
        "# Drop redundant ID columns after merging hcpcsevents\n",
        "merged_df = merged_df.drop(columns=['hcpcs_subject_id', 'hcpcs_hadm_id'])\n",
        "# Delete filtered dataframe after merge\n",
        "del hcpcsevents_merged_df_filtered\n",
        "\n",
        "# Display the head and info of the current merged dataframe\n",
        "print(\"Head of the merged dataframe after adding hcpcsevents:\")\n",
        "display(merged_df.head())\n",
        "\n",
        "print(\"\\nInfo of the merged dataframe after adding hcpcsevents:\")\n",
        "merged_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc4110a0"
      },
      "source": [
        "# Merge with filtered procedures_merged_df\n",
        "merged_df = pd.merge(merged_df, procedures_merged_df_filtered,\n",
        "                                how='left',\n",
        "                                left_on=['diag_subject_id', 'diag_hadm_id'],\n",
        "                                right_on=['proc_subject_id', 'proc_hadm_id'])\n",
        "\n",
        "# Drop redundant ID columns after merging procedures\n",
        "merged_df = merged_df.drop(columns=['proc_subject_id', 'proc_hadm_id'])\n",
        "# Delete filtered dataframe after merge\n",
        "del procedures_merged_df_filtered\n",
        "\n",
        "# Display the head and info of the final merged dataframe\n",
        "print(\"Head of the final merged dataframe with sample:\")\n",
        "display(merged_df.head())\n",
        "\n",
        "print(\"\\nInfo of the final merged dataframe with sample:\")\n",
        "merged_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "874a3d47"
      },
      "source": [
        "# Rename columns\n",
        "merged_df = merged_df.rename(columns={\n",
        "    \"diag_subject_id\": \"subject_id\",\n",
        "    \"diag_hadm_id\": \"hadm_id\",\n",
        "    \"admit_admittime\": \"admittime\",\n",
        "    \"admit_dischtime\": \"dischtime\"\n",
        "})\n",
        "\n",
        "# Define the desired column order\n",
        "desired_column_order = [\n",
        "    \"subject_id\",\n",
        "    \"hadm_id\",\n",
        "    \"admittime\",\n",
        "    \"dischtime\",\n",
        "    \"diag_icd_code\",\n",
        "    \"diag_icd_version\",\n",
        "    \"diag_long_title\",\n",
        "    \"hcpcs_chartdate\",\n",
        "    \"hcpcs_cd\",\n",
        "    \"hcpcs_category\",\n",
        "    \"hcpcs_short_description\",\n",
        "    \"hcpcs_long_description\",\n",
        "    \"proc_chartdate\",\n",
        "    \"proc_icd_code\",\n",
        "    \"proc_icd_version\",\n",
        "    \"proc_long_title\",\n",
        "    \"presc_drug\",\n",
        "    \"presc_starttime\"\n",
        "]\n",
        "\n",
        "# Reindex the dataframe to apply the desired column order\n",
        "# Ensure all desired columns exist in the dataframe before reindexing\n",
        "existing_columns_in_order = [col for col in desired_column_order if col in merged_df.columns]\n",
        "\n",
        "merged_df = merged_df[existing_columns_in_order]\n",
        "\n",
        "\n",
        "print(\"Head of the dataframe after renaming and reordering columns:\")\n",
        "display(merged_df.head())\n",
        "\n",
        "print(\"\\nInfo of the dataframe after renaming and reordering columns:\")\n",
        "merged_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "08hNkh2HygYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merged_df Descriptive Statistics & Missing Values"
      ],
      "metadata": {
        "id": "a8vNAI5K5CEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Head of the merged_df:\")\n",
        "display(merged_df.head())\n",
        "\n",
        "print(\"\\nInfo of the merged_df:\")\n",
        "merged_df.info()\n",
        "\n",
        "print(\"\\nDescriptive statistics for numerical columns:\")\n",
        "display(merged_df.describe())\n",
        "\n",
        "print(\"\\nMissing values per column:\")\n",
        "display(merged_df.isnull().sum())"
      ],
      "metadata": {
        "id": "oIqPC_ecyoSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5b2c579"
      },
      "source": [
        "## Explore Categorical Columns and Visualize Distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fb32395"
      },
      "source": [
        "print(\"\\nValue counts for 'diag_icd_code' (Top 20):\")\n",
        "display(merged_df['diag_icd_code'].value_counts().head(20))\n",
        "\n",
        "print(\"\\nValue counts for 'diag_long_title' (Top 20):\")\n",
        "display(merged_df['diag_long_title'].value_counts().head(20))\n",
        "\n",
        "print(\"\\nValue counts for 'presc_drug' (Top 20):\")\n",
        "display(merged_df['presc_drug'].value_counts().head(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff8ef069"
      },
      "source": [
        "# 3. Visualize distributions\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Distribution of 'diag_icd_version'\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=merged_df, x='diag_icd_version')\n",
        "plt.title('Distribution of diag_icd_version')\n",
        "plt.xlabel('ICD Version')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Top N 'diag_icd_code'\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_codes = merged_df['diag_icd_code'].value_counts().nlargest(10)\n",
        "sns.barplot(x=top_codes.index, y=top_codes.values)\n",
        "plt.title('Top 10 diag_icd_code Counts')\n",
        "plt.xlabel('ICD Code')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Top N 'presc_drug'\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_drugs = merged_df['presc_drug'].value_counts().nlargest(10)\n",
        "sns.barplot(x=top_drugs.index, y=top_drugs.values)\n",
        "plt.title('Top 10 presc_drug Counts')\n",
        "plt.xlabel('Drug Name')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Top N 'hcpcs_cd'\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_hcpcs = merged_df['hcpcs_cd'].value_counts().nlargest(10)\n",
        "sns.barplot(x=top_hcpcs.index, y=top_hcpcs.values)\n",
        "plt.title('Top 10 hcpcs_cd Counts')\n",
        "plt.xlabel('HCPCS Code')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Top N 'proc_icd_code'\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_procs = merged_df['proc_icd_code'].value_counts().nlargest(10)\n",
        "sns.barplot(x=top_procs.index, y=top_procs.values)\n",
        "plt.title('Top 10 proc_icd_code Counts')\n",
        "plt.xlabel('Procedure ICD Code')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "730a1f2f"
      },
      "source": [
        "## Explore Relationships Between Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0f1edca"
      },
      "source": [
        "# Relationship between diag_icd_code and presc_drug\n",
        "print(\"\\nRelationship between diag_icd_code and presc_drug (Top 10 combinations):\")\n",
        "# Create a contingency table of the top combinations\n",
        "diag_drug_counts = merged_df.groupby(['diag_icd_code', 'presc_drug']).size().reset_index(name='count')\n",
        "# Sort and display the top combinations\n",
        "display(diag_drug_counts.sort_values(by='count', ascending=False).head(10))\n",
        "\n",
        "# Relationship between diag_icd_code and hcpcs_cd\n",
        "print(\"\\nRelationship between diag_icd_code and hcpcs_cd (Top 10 combinations):\")\n",
        "diag_hcpcs_counts = merged_df.groupby(['diag_icd_code', 'hcpcs_cd']).size().reset_index(name='count')\n",
        "display(diag_hcpcs_counts.sort_values(by='count', ascending=False).head(10))\n",
        "\n",
        "# Relationship between diag_icd_code and proc_icd_code\n",
        "print(\"\\nRelationship between diag_icd_code and proc_icd_code (Top 10 combinations):\")\n",
        "diag_proc_counts = merged_df.groupby(['diag_icd_code', 'proc_icd_code']).size().reset_index(name='count')\n",
        "display(diag_proc_counts.sort_values(by='count', ascending=False).head(10))\n",
        "\n",
        "# Relationship between presc_drug and hcpcs_cd\n",
        "print(\"\\nRelationship between presc_drug and hcpcs_cd (Top 10 combinations):\")\n",
        "drug_hcpcs_counts = merged_df.groupby(['presc_drug', 'hcpcs_cd']).size().reset_index(name='count')\n",
        "display(drug_hcpcs_counts.sort_values(by='count', ascending=False).head(10))\n",
        "\n",
        "\n",
        "# Relationship between presc_drug and proc_icd_code\n",
        "print(\"\\nRelationship between presc_drug and proc_icd_code (Top 10 combinations):\")\n",
        "drug_proc_counts = merged_df.groupby(['presc_drug', 'proc_icd_code']).size().reset_index(name='count')\n",
        "display(drug_proc_counts.sort_values(by='count', ascending=False).head(10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CKD Guideline Recommended Treatmemt by Category & Dialysis ICD Codes"
      ],
      "metadata": {
        "id": "MpkUdM3e5qMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the drug categories\n",
        "ckd_grt = {\n",
        "    \"SGLT2 inhibitor\": [\"empagliflozin\", \"Empagliflozin\", \"jardiance\", \"Jardiance\", \"Jardiance (empagliflozin)\", \"Jardiance (Empaglifozin)\",\n",
        "                        \"Jardiance 10mg\", \"Jardiance(empagliflozin)\", \"Jardiance(Empagliflozin)\", \"empagliflozin-linagliptin (GLYXAMBI)\", \"INV-Empagliflozin\",\n",
        "                        \"dapagliflozin\", \"Dapagliflozin\", \"farxiga\", \"Farxiga\", \"linagliptin-empagliflozin (GLYXAMBI)\",\n",
        "                        \"canagliflozin\", \"Canagliflozin\", \"invokana\", \"Invokana\", \"invokana (canagliflozin)\",\n",
        "                        \"ertugliflozin\", \"Ertugliflozin\", \"Steglatro\", \"steglatro\", \"steglatro (ertugliflozin)\",\n",
        "                        \"bexagliflozin\", \"Bexagliflozin\", \"Brenzavvy\", \"brenzavvy\", \"brenzavvy (bexagliflozin)\",\n",
        "                        \"sotagliflozin\", \"Sotagliflozin\", \"Inpefa\", \"inpefa\", \"inpefa (sotagliflozin)\"],\n",
        "    \"ACE inhibitor\": [\"lisinopril\", \"Lisinopril\", \"lisinopril-hydrochlorothiazide\", \"qbrelis\", \"Qbrelis\", \"zestril\", \"Zestril\", \"Zestril (lisinopril)\",\n",
        "                      \"ramipril\", \"Ramipril\", \"Altace\", \"altace\",\n",
        "                      \"captopril\", \"Captopril\", \"Capoten\", \"capoten\", \"Capoten (captopril)\", \"captopril-hydrochlorothiazide\",\n",
        "                      \"perindopril\", \"Perindopril\", \"Perindopril Sodium\", \"Perindopril-hydrochlorothiazide\", \"aceon\", \"Aceon\", \"coversyl\", \"Coversyl\", \"coversum\", \"Coversum\", \"prexum\", \"Prexum\", \"prestarium\", \"Prestarium\",\n",
        "                      \"benazepril\", \"Benazepril\", \"Lotensin\", \"lotensin\", \"Lotensin Sodium\", \"Lotensin-hydrochlorothiazide\", \"Lotensin (benazepril)\",\n",
        "                      \"fosinopril\", \"Fosinopril\", \"Fosinopril Sodium\", \"Fosinopril-hydrochlorothiazide\", \"Fosinopril (fosinopril sodium)\",\n",
        "                      \"moexipril\", \"Moexipril\", \"Moexipril Sodium\", \"Moexipril-hydrochlorothiazide\", \"Moexipril (moexipril sodium)\",\n",
        "                      \"quinapril\", \"Quinapril\", \"Quinapril Maleate\", \"Quinapril Sodium\", \"Quinapril Sodium (quinapril)\", \"INV-Quinapril\",\n",
        "                      \"trandolapril\", \"Trandolapril\", \"Tramadol\", \"Tramadol Sodium\", \"Tramadol-hydrochlorothiazide\",\n",
        "                      \"enalapril\", \"Enalapril\", \"Enalapril Maleate\", \"enalapril maleate\", \"Enalaprilat\", \"INV-Enalapril\", \"Vasotec (enalapril)\", \"vasotec\"],\n",
        "    \"ARBs\": [\"losartan\", \"Losartan\", \"Losartan Potassium\", \"Cozaar\", \"cozaar\",\n",
        "             \"Hyzaar\", \"losartan-hydrochlorothiazide\",\n",
        "             \"azilsartan\", \"Azilsartan\", \"Edarbi\", \"edarbi\",\n",
        "             \"candesartan\", \"Candesartan\", \"Atacand\", \"atacand\", \"atacand (candesartan)\",\n",
        "             \"irbesartan\", \"Irbesartan\", \"Avapro\", \"avapro\",\n",
        "             \"olmesartan\", \"Olmesartan\", \"Benicar\", \"benicar\",\n",
        "             \"telmisartan\", \"Telmisartan\", \"Micardis\", \"micardis\",\n",
        "             \"valsartan\", \"Valsartan\", \"Diovan\", \"diovan\", \"valsartan sodium\",\n",
        "             \"eprosartan\", \"Eprosartan\", \"Teveten\", \"teveten\"],\n",
        "    \"Statin\": [\"Atorvastatin\", \"atorvastatin\", \"atorvastatin\", \"atorvastatin 40 mg\",\n",
        "               \"Atorvastatin 40 mg\", \"Atorvastatin 40 mg\", \"atorvastatin 40 mg cap\",\n",
        "               \"atorvastatin 40 mg capsule\", \"Atorvastatin 40 mg capsule\", \"atorvastatin 40 mg capsule\",\n",
        "               \"Atorvastatin 40mg\", \"Atorvastatin 40mg Tab\", \"INV-Atorvastatin\", \"Lipitor\", \"lipitor\",\n",
        "               \"Fluvastatin\", \"fluvastatin\", \"Fluvastatin Sodium\",  \"Lescol XL (fluvastatin XL)\", \"Lescol XL\", \"Lescol\",\n",
        "               \"Lovastatin\", \"lovastatin\", \"Altoprev\", \"Lovastatin 10mg\", \"Lovastatin 20mg\", \"Mevacor\",\n",
        "               \"pitavastatin\", \"pitavastatin calcium\", \"pitavastatin calcium (Livalo)\", \"Livalo (pitavastatin calcium)\", \"Livalo\", \"Zypitamag\",\n",
        "               \"Pravastatin\", \"pravastatin\", \"Pravastatin Sodium\", \"Pravastatin Sodium 10mg\",\n",
        "               \"rosuvastatin\", \"Rosuvastatin\", \"Rosuvastatin Calcium\", \"Rosuvastatin Calcium 10mg\", \"Crestor\", \"crestor\",\n",
        "               \"Simvastatin\", \"simvastatin\", \"SImvastatin\", \"simvastatin\", \"Simvastatin\", \"simvastatin 40 mg\", \"Simvastatin 40 mg\", \"Zocor\", \"zocor\"]\n",
        "}\n",
        "\n",
        "print(\"Defined CKD Guideline Recommended Treatment drug categories:\")\n",
        "display(ckd_grt)"
      ],
      "metadata": {
        "id": "hDK2LA7W5wOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuE62KGTWTYp"
      },
      "source": [
        "# ICD-9 Procedure Codes: 39.95 (Hemodialysis), 39.99 (Other extracorporeal procedures)\n",
        "# ICD-10 Procedure Codes: 5A1D000 (Hemodialysis), 5A1900W (Continuous renal replacement therapy [CRRT])\n",
        "# HCPCS Codes: G0490-G0499 (ESRD related services), 90935-90970 (Hemodialysis services)\n",
        "dialysis_proc_icd_codes = ['3995', '3999', '5A1D000', '5A1900W', '5498']\n",
        "dialysis_hcpcs_codes = [f'G049{i}' for i in range(10)] + [f'909{i:02d}' for i in range(35, 71)] + ['G0257']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd0597b9"
      },
      "source": [
        "# Define the mapping of original codes to new categories\n",
        "diagnosis_mapping = {\n",
        "    'Hypertensive CKD (Stages 1-4)': ['I129', 'I130', '40390', 'I1310', '40310', '40491', '40300', '40490', '40411', '40401'],\n",
        "    'Hypertensive CKD (Stages 5-End Stage)': ['40311', 'I120', '40391', 'I132', '40301', 'I1311', '40493', '40492', '40403'],\n",
        "    'Diabetes with CKD': ['E1122', 'E1022', 'E0922', 'E1322', 'E0822'],\n",
        "    'Stage 1 CKD': ['5851', 'N181'],\n",
        "    'Stage 2 CKD': ['N182'],\n",
        "    'Stage 3 CKD': ['N183', 'N1830', '5853', '5852', 'N1832'],\n",
        "    'Stage 4 CKD': ['5854', 'N184'],\n",
        "    'Stage 5 CKD': ['5855', 'N185'],\n",
        "    'Unspecified CKD': ['N189', '5859']\n",
        "}\n",
        "\n",
        "# Create a reverse mapping from code to category\n",
        "code_to_category = {}\n",
        "for category, codes in diagnosis_mapping.items():\n",
        "    for code in codes:\n",
        "        code_to_category[code] = category\n",
        "\n",
        "# Apply the mapping to create the new 'grouped_diagnosis' column in merged_df\n",
        "merged_df['grouped_diagnosis'] = merged_df['diag_icd_code'].map(code_to_category)\n",
        "\n",
        "# Check if there are any codes in the dataframe that were not in the mapping\n",
        "unmapped_codes = merged_df[merged_df['grouped_diagnosis'].isna()]['diag_icd_code'].unique()\n",
        "if unmapped_codes.size > 0:\n",
        "    print(f\"\\nWarning: The following diag_icd_codes were not found in the provided mapping and will have NaN in 'grouped_diagnosis': {list(unmapped_codes)}\")\n",
        "\n",
        "\n",
        "print(\"\\nMerged_df with new grouped_diagnosis column:\")\n",
        "display(merged_df.head())\n",
        "merged_df.info()\n",
        "\n",
        "print(\"\\nValue counts for the new grouped_diagnosis column in merged_df:\")\n",
        "display(merged_df['grouped_diagnosis'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcd53bc0"
      },
      "source": [
        "# Survival Analysis Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59703b63"
      },
      "source": [
        "## Data preparation for survival analysis\n",
        "\n",
        "Prepare the `merged_df` for survival analysis by defining the event (dialysis) and the time to event (time from treatment start to dialysis or end of observation). This will involve identifying the first occurrence of a CKD guideline-recommended treatment and the first occurrence of a dialysis event for each subject.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93f5dcdc"
      },
      "source": [
        "# 1. Create a new column in merged_df called is_dialysis\n",
        "# Check if proc_icd_code or hcpcs_cd is in the dialysis code lists\n",
        "merged_df['is_dialysis'] = 0\n",
        "merged_df.loc[merged_df['proc_icd_code'].isin(dialysis_proc_icd_codes), 'is_dialysis'] = 1\n",
        "merged_df.loc[merged_df['hcpcs_cd'].isin(dialysis_hcpcs_codes), 'is_dialysis'] = 1\n",
        "\n",
        "print(\"Added 'is_dialysis' column to merged_df:\")\n",
        "display(merged_df[['proc_icd_code', 'hcpcs_cd', 'is_dialysis']].head())\n",
        "\n",
        "\n",
        "# 3. Define a list of all CKD GRT drugs\n",
        "ckd_grt_drugs = [drug for sublist in ckd_grt.values() for drug in sublist]\n",
        "\n",
        "# 4. Filter prescriptions to only include CKD GRT drugs\n",
        "# Need to access the original prescriptions_df from dataframe_dict to get all prescriptions\n",
        "prescriptions_df = dataframe_dict['prescriptions.csv']\n",
        "ckd_grt_prescriptions = prescriptions_df[prescriptions_df['presc_drug'].str.lower().isin([d.lower() for d in ckd_grt_drugs])].copy()\n",
        "\n",
        "\n",
        "# 5. Find the earliest presc_starttime for CKD guideline-recommended treatments for each subject\n",
        "treatment_start_times = ckd_grt_prescriptions.groupby('presc_subject_id')['presc_starttime'].min().reset_index()\n",
        "treatment_start_times = treatment_start_times.rename(columns={'presc_subject_id': 'subject_id', 'presc_starttime': 'treatment_starttime'})\n",
        "\n",
        "print(\"\\nEarliest treatment start time for each subject:\")\n",
        "display(treatment_start_times.head())\n",
        "\n",
        "\n",
        "# 6. Filter merged_df for dialysis events\n",
        "dialysis_events_df = merged_df[merged_df['is_dialysis'] == 1].copy()\n",
        "\n",
        "# 7. Combine potential dialysis dates from procedures and hcpcs events\n",
        "# Ensure chartdate columns are datetime objects\n",
        "dialysis_events_df['proc_chartdate'] = pd.to_datetime(dialysis_events_df['proc_chartdate'])\n",
        "dialysis_events_df['hcpcs_chartdate'] = pd.to_datetime(dialysis_events_df['hcpcs_chartdate'])\n",
        "dialysis_events_df['event_date'] = dialysis_events_df[['proc_chartdate', 'hcpcs_chartdate']].min(axis=1)\n",
        "\n",
        "\n",
        "# 8. Find the earliest event date for each subject\n",
        "# Use the correct subject_id column name from merged_df\n",
        "dialysis_event_dates = dialysis_events_df.groupby('subject_id')['event_date'].min().reset_index()\n",
        "dialysis_event_dates = dialysis_event_dates.rename(columns={'event_date': 'dialysis_event_date'})\n",
        "\n",
        "\n",
        "print(\"\\nEarliest dialysis event date for each subject:\")\n",
        "display(dialysis_event_dates.head())\n",
        "\n",
        "# 9. Create a new DataFrame at the subject level\n",
        "# Start with unique subject_ids from the filtered diagnoses (using the original diag_subject_id before rename)\n",
        "subject_survival_df = filtered_diagnoses_merged_df[['diag_subject_id']].drop_duplicates().rename(columns={'diag_subject_id': 'subject_id'})\n",
        "\n",
        "\n",
        "# 10. Merge with treatment start times\n",
        "subject_survival_df = pd.merge(subject_survival_df, treatment_start_times, on='subject_id', how='left')\n",
        "\n",
        "# 11. Merge with dialysis event dates\n",
        "subject_survival_df = pd.merge(subject_survival_df, dialysis_event_dates, on='subject_id', how='left')\n",
        "\n",
        "print(\"\\nSubject-level survival DataFrame before calculating time to event:\")\n",
        "display(subject_survival_df.head())\n",
        "subject_survival_df.info()\n",
        "\n",
        "# 12. Define the end of the observation period\n",
        "end_of_observation = merged_df['admittime'].max() # Using admittime as a proxy for study end\n",
        "\n",
        "# 13. Fill missing dialysis_event_date with the end of observation date for censored subjects\n",
        "subject_survival_df['dialysis_event_date'] = subject_survival_df['dialysis_event_date'].fillna(end_of_observation)\n",
        "\n",
        "# 14. Calculate time_to_event in days\n",
        "subject_survival_df['time_to_event'] = (subject_survival_df['dialysis_event_date'] - subject_survival_df['treatment_starttime']).dt.days\n",
        "\n",
        "# 15. Create event column (1 if dialysis_event_date is not the end_of_observation date, 0 otherwise)\n",
        "subject_survival_df['event'] = (subject_survival_df['dialysis_event_date'] != end_of_observation).astype(int)\n",
        "\n",
        "# 16. Drop rows where treatment_starttime is missing (subjects who didn't receive GRT treatment in the dataset)\n",
        "subject_survival_df = subject_survival_df.dropna(subset=['treatment_starttime']).copy()\n",
        "\n",
        "\n",
        "print(\"\\nSubject-level survival DataFrame with time_to_event and event:\")\n",
        "display(subject_survival_df.head())\n",
        "subject_survival_df.info()\n",
        "\n",
        "\n",
        "# 17. Create the final survival analysis DataFrame\n",
        "survival_analysis_df = subject_survival_df[['subject_id', 'treatment_starttime', 'dialysis_event_date', 'time_to_event', 'event']].copy()\n",
        "\n",
        "# 18. Get unique grouped diagnoses per subject from the original merged_df\n",
        "# Use the subject_id column name from merged_df\n",
        "subject_grouped_diagnoses = merged_df[['subject_id', 'grouped_diagnosis']].drop_duplicates()\n",
        "\n",
        "\n",
        "# 19. Merge unique grouped diagnoses into the survival dataframe\n",
        "survival_analysis_df = pd.merge(survival_analysis_df, subject_grouped_diagnoses, on='subject_id', how='left')\n",
        "\n",
        "# 20. Drop rows where grouped_diagnosis is missing (from the merge if a subject had no grouped diagnosis)\n",
        "survival_analysis_df = survival_analysis_df.dropna(subset=['grouped_diagnosis']).copy()\n",
        "\n",
        "# 21. Delete intermediate dataframes to free up memory\n",
        "del treatment_start_times\n",
        "del dialysis_event_dates\n",
        "del subject_survival_df\n",
        "del ckd_grt_prescriptions\n",
        "del dialysis_events_df\n",
        "del subject_grouped_diagnoses\n",
        "del prescriptions_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4efa9c96"
      },
      "source": [
        "## Feature engineering\n",
        "Create features based on the CKD guideline-recommended treatments, considering whether a patient is on *any* such treatment, or perhaps indicators for specific classes of treatments (SGLT2 inhibitors, ACE inhibitors, etc.).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e402017c"
      },
      "source": [
        "# 1. Create a binary feature for whether a subject received ANY CKD guideline-recommended treatment.\n",
        "# This is implicitly handled by the fact that we dropped rows with missing 'treatment_starttime'\n",
        "# in the previous subtask. The presence of a row in survival_analysis_df means they had a\n",
        "# treatment_starttime, indicating they received at least one GRT.\n",
        "\n",
        "# 2. For each category of CKD guideline-recommended treatment, create a binary feature.\n",
        "# Get a fresh copy of merged_df to avoid modifying it with temporary columns\n",
        "temp_merged_df = merged_df.copy()\n",
        "\n",
        "# Create a list of all CKD GRT drugs (case-insensitive)\n",
        "ckd_grt_drugs_lower = [drug.lower() for sublist in ckd_grt.values() for drug in sublist]\n",
        "\n",
        "# Filter prescriptions in the temporary merged_df to only include CKD GRT drugs\n",
        "# and ensure 'presc_drug' column is treated case-insensitively.\n",
        "ckd_grt_prescriptions_temp = temp_merged_df[temp_merged_df['presc_drug'].str.lower().isin(ckd_grt_drugs_lower)].copy()\n",
        "\n",
        "# Filter these prescriptions to keep only those that occurred at or after the subject's\n",
        "# earliest treatment_starttime. This requires merging with survival_analysis_df to get the start time.\n",
        "category_prescriptions_after_start = pd.merge(\n",
        "    ckd_grt_prescriptions_temp,\n",
        "    survival_analysis_df[['subject_id', 'treatment_starttime']],\n",
        "    on='subject_id',\n",
        "    how='inner' # Use inner merge to keep only subjects in survival_analysis_df\n",
        ")\n",
        "\n",
        "# Filter prescriptions that are on or after the treatment_starttime\n",
        "category_prescriptions_after_start = category_prescriptions_after_start[\n",
        "    category_prescriptions_after_start['presc_starttime'] >= category_prescriptions_after_start['treatment_starttime']\n",
        "].copy()\n",
        "\n",
        "# Create binary features for each category\n",
        "for category, drugs in ckd_grt.items():\n",
        "    # Get the drugs for the current category (case-insensitive)\n",
        "    category_drugs_lower = [drug.lower() for drug in drugs]\n",
        "\n",
        "    # Filter prescriptions after start time for drugs in this category\n",
        "    subjects_on_category_treatment_df = category_prescriptions_after_start[\n",
        "        category_prescriptions_after_start['presc_drug'].str.lower().isin(category_drugs_lower)\n",
        "    ].copy()\n",
        "\n",
        "    # Identify subjects who received at least one drug from this category after their treatment_starttime\n",
        "    subjects_on_category_treatment = subjects_on_category_treatment_df['subject_id'].unique()\n",
        "\n",
        "    # Create the binary feature column in survival_analysis_df\n",
        "    feature_name = f'on_{category.lower().replace(\" \", \"_\")}'\n",
        "    survival_analysis_df[feature_name] = survival_analysis_df['subject_id'].isin(subjects_on_category_treatment).astype(int)\n",
        "\n",
        "# 3. Display the head of survival_analysis_df after adding the new features.\n",
        "print(\"Added binary features for CKD guideline-recommended treatment categories to survival_analysis_df:\")\n",
        "display(survival_analysis_df.head())\n",
        "\n",
        "# 4. Display the info of survival_analysis_df after adding new features.\n",
        "print(\"\\nInfo of survival_analysis_df after adding treatment category features:\")\n",
        "survival_analysis_df.info()\n",
        "\n",
        "# 5. Display value counts for each of the newly created binary treatment category features.\n",
        "print(\"\\nValue counts for new treatment category features:\")\n",
        "for category in ckd_grt.keys():\n",
        "    feature_name = f'on_{category.lower().replace(\" \", \"_\")}'\n",
        "    print(f\"\\nValue counts for '{feature_name}':\")\n",
        "    display(survival_analysis_df[feature_name].value_counts())\n",
        "\n",
        "# 6. Delete intermediate dataframes to free up memory.\n",
        "del temp_merged_df\n",
        "del ckd_grt_prescriptions_temp\n",
        "del category_prescriptions_after_start\n",
        "del subjects_on_category_treatment_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab3e1535"
      },
      "source": [
        "# Check SGLT2 binary entries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bade58a7"
      },
      "source": [
        "# 1. Access the prescriptions.csv DataFrame from dataframe_dict\n",
        "raw_prescriptions_df = dataframe_dict['prescriptions.csv'].copy()\n",
        "\n",
        "# 2. Extract the list of SGLT2 inhibitor drugs from the ckd_grt dictionary\n",
        "sglt2_inhibitors = ckd_grt['SGLT2 inhibitor']\n",
        "sglt2_inhibitors_lower = [drug.lower() for drug in sglt2_inhibitors]\n",
        "\n",
        "# 3. Create a boolean mask for SGLT2 inhibitor drugs (case-insensitive)\n",
        "sglt2_mask = raw_prescriptions_df['presc_drug'].str.lower().isin(sglt2_inhibitors_lower)\n",
        "\n",
        "# 4. Filter raw_prescriptions_df using this mask\n",
        "sglt2_prescriptions_raw_df = raw_prescriptions_df[sglt2_mask].copy()\n",
        "\n",
        "print(\"SGLT2 Inhibitor Prescriptions in Raw Data (Head):\")\n",
        "# 5. Print the head of this filtered DataFrame\n",
        "display(sglt2_prescriptions_raw_df.head())\n",
        "\n",
        "print(\"\\nInfo of SGLT2 Inhibitor Prescriptions in Raw Data:\")\n",
        "# 6. Print the info of this filtered DataFrame\n",
        "sglt2_prescriptions_raw_df.info()\n",
        "\n",
        "print(\"\\nValue Counts for SGLT2 Inhibitor Drugs in Raw Data:\")\n",
        "# 7. Display the value counts of the 'presc_drug' column\n",
        "display(sglt2_prescriptions_raw_df['presc_drug'].value_counts())\n",
        "\n",
        "del raw_prescriptions_df # Clean up temporary DataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fd3753e"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Get unique subject_ids from the raw SGLT2 inhibitor prescriptions\n",
        "sglt2_subjects_raw = sglt2_prescriptions_raw_df['presc_subject_id'].unique()\n",
        "\n",
        "# 2. Filter survival_analysis_df to include only these subjects\n",
        "filtered_survival_for_sglt2 = survival_analysis_df[\n",
        "    survival_analysis_df['subject_id'].isin(sglt2_subjects_raw)\n",
        "].copy()\n",
        "\n",
        "# 3. Merge sglt2_prescriptions_raw_df with filtered_survival_for_sglt2\n",
        "#    on subject_id to bring in the treatment_starttime for these subjects.\n",
        "\n",
        "merged_sglt2_check = pd.merge(\n",
        "    sglt2_prescriptions_raw_df.rename(columns={'presc_subject_id': 'subject_id'}),\n",
        "    filtered_survival_for_sglt2[['subject_id', 'treatment_starttime']],\n",
        "    on='subject_id',\n",
        "    how='inner' # Keep only subjects present in both\n",
        ")\n",
        "\n",
        "# 4. Check if presc_starttime (of the SGLT2 inhibitor) is >= treatment_starttime\n",
        "merged_sglt2_check['is_sglt2_after_treatment_start'] = (\n",
        "    merged_sglt2_check['presc_starttime'] >= merged_sglt2_check['treatment_starttime']\n",
        ")\n",
        "\n",
        "print(\"Head of merged data for SGLT2 check with treatment_starttime:\")\n",
        "display(merged_sglt2_check.head())\n",
        "\n",
        "# 5. Count how many unique subjects have at least one SGLT2 inhibitor prescription\n",
        "#    that occurred at or after their overall GRT treatment_starttime.\n",
        "subjects_with_valid_sglt2_post_start = merged_sglt2_check[\n",
        "    merged_sglt2_check['is_sglt2_after_treatment_start']\n",
        "]['subject_id'].nunique()\n",
        "\n",
        "print(f\"\\nNumber of unique subjects with SGLT2 inhibitor prescription >= overall GRT treatment_starttime: {subjects_with_valid_sglt2_post_start}\")\n",
        "\n",
        "# 6. Compare this with the count of '1's in the 'on_sglt2_inhibitor' column in survival_analysis_df\n",
        "actual_on_sglt2_count = survival_analysis_df['on_sglt2_inhibitor'].sum()\n",
        "\n",
        "print(f\"Number of subjects marked 'on_sglt2_inhibitor' in survival_analysis_df: {actual_on_sglt2_count}\")\n",
        "\n",
        "if subjects_with_valid_sglt2_post_start == actual_on_sglt2_count:\n",
        "    print(\"\\nThe counts match, indicating the logic for 'on_sglt2_inhibitor' is consistent with prescriptions after treatment_starttime.\")\n",
        "else:\n",
        "    print(\"\\nThe counts do NOT match, indicating a potential discrepancy in how 'on_sglt2_inhibitor' was created or filtered.\")\n",
        "\n",
        "# Clean up temporary DataFrame\n",
        "del merged_sglt2_check\n",
        "del filtered_survival_for_sglt2\n",
        "del sglt2_subjects_raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4eee3c1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Extract the list of SGLT2 inhibitor drugs from the ckd_grt dictionary (case-insensitive)\n",
        "sglt2_inhibitors = ckd_grt['SGLT2 inhibitor']\n",
        "sglt2_inhibitors_lower = [drug.lower() for drug in sglt2_inhibitors]\n",
        "\n",
        "# Check if any SGLT2 inhibitor drugs are present in the 'presc_drug' column of merged_df\n",
        "# Note: merged_df contains one row per diagnosis, with one associated prescription if available\n",
        "sglt2_in_merged_df = merged_df[merged_df['presc_drug'].str.lower().isin(sglt2_inhibitors_lower)].copy()\n",
        "\n",
        "print(\"Head of merged_df rows containing SGLT2 inhibitors:\")\n",
        "display(sglt2_in_merged_df.head())\n",
        "\n",
        "print(\"\\nNumber of rows in merged_df containing SGLT2 inhibitors:\" \\\n",
        "      f\" {len(sglt2_in_merged_df)}\")\n",
        "\n",
        "# Get the number of unique subjects from these rows\n",
        "unique_subjects_with_sglt2_in_merged_df = sglt2_in_merged_df['subject_id'].nunique()\n",
        "print(\"Number of unique subjects in merged_df with SGLT2 inhibitors:\"\\\n",
        "      f\" {unique_subjects_with_sglt2_in_merged_df}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5de94a96"
      },
      "source": [
        "## Analyze SGLT2 Inhibitor Presence in ckd_grt_prescriptions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77507bd1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Re-create ckd_grt_prescriptions as it was deleted previously\n",
        "prescriptions_df = dataframe_dict['prescriptions.csv']\n",
        "ckd_grt_drugs = [drug for sublist in ckd_grt.values() for drug in sublist]\n",
        "ckd_grt_prescriptions = prescriptions_df[prescriptions_df['presc_drug'].str.lower().isin([d.lower() for d in ckd_grt_drugs])].copy()\n",
        "\n",
        "# 1. Extract the list of SGLT2 inhibitor drugs from the ckd_grt dictionary\n",
        "sglt2_inhibitors = ckd_grt['SGLT2 inhibitor']\n",
        "sglt2_inhibitors_lower = [drug.lower() for drug in sglt2_inhibitors]\n",
        "\n",
        "# 2. Filter ckd_grt_prescriptions using a case-insensitive match for SGLT2 inhibitors\n",
        "sglt2_prescriptions_filtered_ckd_grt = ckd_grt_prescriptions[\n",
        "    ckd_grt_prescriptions['presc_drug'].str.lower().isin(sglt2_inhibitors_lower)\n",
        "].copy()\n",
        "\n",
        "print(\"SGLT2 Inhibitor Prescriptions in ckd_grt_prescriptions (Head):\")\n",
        "# 3. Display the head of this filtered DataFrame\n",
        "display(sglt2_prescriptions_filtered_ckd_grt.head())\n",
        "\n",
        "print(\"\\nInfo of SGLT2 Inhibitor Prescriptions in ckd_grt_prescriptions:\")\n",
        "# 4. Print the .info() of this filtered DataFrame\n",
        "sglt2_prescriptions_filtered_ckd_grt.info()\n",
        "\n",
        "print(\"\\nValue Counts for SGLT2 Inhibitor Drugs in ckd_grt_prescriptions:\")\n",
        "# 5. Display the value counts of the 'presc_drug' column\n",
        "display(sglt2_prescriptions_filtered_ckd_grt['presc_drug'].value_counts())\n",
        "\n",
        "# Clean up temporary dataframes\n",
        "del prescriptions_df\n",
        "del ckd_grt_prescriptions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b802809"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Re-create prescriptions_df_filtered if it was deleted\n",
        "prescriptions_df = dataframe_dict['prescriptions.csv'].copy()\n",
        "unique_ids = filtered_diagnoses_merged_df[['diag_subject_id', 'diag_hadm_id']].drop_duplicates()\n",
        "prescriptions_df_filtered_recreated = pd.merge(\n",
        "    unique_ids,\n",
        "    prescriptions_df.rename(columns={'presc_subject_id': 'diag_subject_id', 'presc_hadm_id': 'diag_hadm_id'}),\n",
        "    on=['diag_subject_id', 'diag_hadm_id'],\n",
        "    how='left'\n",
        ")\n",
        "prescriptions_df_filtered_recreated = prescriptions_df_filtered_recreated.rename(columns={'diag_subject_id': 'presc_subject_id', 'diag_hadm_id': 'presc_hadm_id'})\n",
        "\n",
        "# Extract the list of SGLT2 inhibitor drugs from the ckd_grt dictionary (case-insensitive)\n",
        "sglt2_inhibitors = ckd_grt['SGLT2 inhibitor']\n",
        "sglt2_inhibitors_lower = [drug.lower() for drug in sglt2_inhibitors]\n",
        "\n",
        "# Filter prescriptions_df_filtered_recreated for SGLT2 inhibitors\n",
        "sglt2_in_prescriptions_filtered = prescriptions_df_filtered_recreated[\n",
        "    prescriptions_df_filtered_recreated['presc_drug'].str.lower().isin(sglt2_inhibitors_lower)\n",
        "].copy()\n",
        "\n",
        "print(\"SGLT2 Inhibitor Prescriptions in prescriptions_df_filtered (Head):\")\n",
        "display(sglt2_in_prescriptions_filtered.head())\n",
        "\n",
        "print(\"\\nNumber of rows with SGLT2 Inhibitor Prescriptions in prescriptions_df_filtered: \" \\\n",
        "      f\"{len(sglt2_in_prescriptions_filtered)}\")\n",
        "\n",
        "print(\"\\nUnique subjects with SGLT2 Inhibitor Prescriptions in prescriptions_df_filtered: \" \\\n",
        "      f\"{sglt2_in_prescriptions_filtered['presc_subject_id'].nunique()}\")\n",
        "\n",
        "# Clean up temporary dataframes\n",
        "del prescriptions_df\n",
        "del prescriptions_df_filtered_recreated\n",
        "del unique_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "404d300d"
      },
      "source": [
        "## Verify SGLT2 inhibitor presence in filtered_diagnoses_merged patients\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "953ce4cb"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Access the filtered_diagnoses_merged_df DataFrame and sglt2_prescriptions_raw_df\n",
        "# These are already available in the kernel state.\n",
        "\n",
        "# 2. Create a DataFrame `unique_sampled_ids`\n",
        "unique_sampled_ids = filtered_diagnoses_merged_df[['diag_subject_id', 'diag_hadm_id']].drop_duplicates().copy()\n",
        "unique_sampled_ids = unique_sampled_ids.rename(columns={'diag_subject_id': 'subject_id', 'diag_hadm_id': 'hadm_id'})\n",
        "\n",
        "# 3. Create a DataFrame `sglt2_unique_presc_ids`\n",
        "sglt2_unique_presc_ids = sglt2_prescriptions_raw_df[['presc_subject_id', 'presc_hadm_id']].drop_duplicates().copy()\n",
        "sglt2_unique_presc_ids = sglt2_unique_presc_ids.rename(columns={'presc_subject_id': 'subject_id', 'presc_hadm_id': 'hadm_id'})\n",
        "\n",
        "# 4. Perform an inner merge to find overlapping (subject, admission) pairs\n",
        "merged_sglt2_in_sampled = pd.merge(\n",
        "    unique_sampled_ids,\n",
        "    sglt2_unique_presc_ids,\n",
        "    on=['subject_id', 'hadm_id'],\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "print(\"--- SGLT2 Inhibitor Presence in Patients ---\")\n",
        "\n",
        "# 5. Print the number of rows in the resulting merged DataFrame\n",
        "num_overlapping_pairs = len(merged_sglt2_in_sampled)\n",
        "print(f\"Number of (subject, admission) pairs from filtered_diagnoses_merged_df with SGLT2 inhibitor prescriptions: {num_overlapping_pairs}\")\n",
        "\n",
        "# 6. Display the head of this merged DataFrame if it contains any entries\n",
        "if num_overlapping_pairs > 0:\n",
        "    print(\"Head of overlapping (subject, admission) pairs:\")\n",
        "    display(merged_sglt2_in_sampled.head())\n",
        "else:\n",
        "    print(\"No overlapping (subject, admission) pairs found.\")\n",
        "\n",
        "# 7. Print the number of unique subject_ids found in this merged DataFrame\n",
        "unique_subjects_with_sglt2_in_sampled = merged_sglt2_in_sampled['subject_id'].nunique()\n",
        "print(f\"Number of unique subjects from filtered_diagnoses_merged_df with SGLT2 inhibitor prescriptions: {unique_subjects_with_sglt2_in_sampled}\")\n",
        "\n",
        "# 8. Delete any temporary DataFrames\n",
        "del unique_sampled_ids\n",
        "del sglt2_unique_presc_ids\n",
        "del merged_sglt2_in_sampled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d645c22"
      },
      "source": [
        "## Address multicollinearity and complete separation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2a10661"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Select predictor columns for correlation analysis\n",
        "# Include the 'on_' treatment features and the one-hot encoded grouped diagnoses.\n",
        "# Exclude the original 'grouped_diagnosis' object column and identifier columns.\n",
        "predictor_cols = [col for col in survival_analysis_df.columns if col.startswith('on_') or col.startswith('diag_')]\n",
        "\n",
        "# Ensure all predictor columns are numeric (convert boolean to int if necessary)\n",
        "correlation_df = survival_analysis_df[predictor_cols].copy()\n",
        "for col in correlation_df.columns:\n",
        "    if correlation_df[col].dtype == 'bool':\n",
        "        correlation_df[col] = correlation_df[col].astype(int)\n",
        "\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = correlation_df.corr()\n",
        "\n",
        "print(\"Correlation matrix for predictor variables (including one-hot encoded diagnoses):\")\n",
        "display(correlation_matrix)\n",
        "\n",
        "# Visualize the correlation matrix using a heatmap\n",
        "plt.figure(figsize=(15, 12)) # Adjusted figure size for more predictors\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", annot_kws={\"size\": 8})\n",
        "plt.title('Correlation Matrix of Predictor Variables')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Identify highly correlated pairs (e.g., absolute correlation > 0.7)\n",
        "print(\"\\nHighly correlated pairs (absolute correlation > 0.7):\")\n",
        "high_corr_pairs = {}\n",
        "# Iterate through the upper triangle of the correlation matrix\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i + 1, len(correlation_matrix.columns)): # Start from i+1 to avoid duplicates and diagonal\n",
        "        col1 = correlation_matrix.columns[i]\n",
        "        col2 = correlation_matrix.columns[j]\n",
        "        corr_value = correlation_matrix.iloc[i, j]\n",
        "        if abs(corr_value) > 0.7:\n",
        "            high_corr_pairs[(col1, col2)] = corr_value\n",
        "\n",
        "if high_corr_pairs:\n",
        "    for pair, corr_value in high_corr_pairs.items():\n",
        "        print(f\"  {pair[0]} and {pair[1]}: {corr_value:.2f}\")\n",
        "else:\n",
        "    print(\"  No highly correlated pairs found (absolute correlation > 0.7).\")\n",
        "\n",
        "# Examine the 'on_arbs' column for complete separation\n",
        "print(\"\\nValue counts for 'on_arbs':\")\n",
        "arbs_value_counts = survival_analysis_df['on_arbs'].value_counts()\n",
        "display(arbs_value_counts)\n",
        "\n",
        "print(\"\\nRelationship between 'on_arbs' and 'event' (for complete separation check):\")\n",
        "arbs_event_crosstab = pd.crosstab(survival_analysis_df['on_arbs'], survival_analysis_df['event'])\n",
        "display(arbs_event_crosstab)\n",
        "\n",
        "# Confirm complete separation if all events (event=1) occur in only one category of 'on_arbs'\n",
        "# Based on previous value counts, it's likely that 'on_arbs' has no '1' values,\n",
        "# which would lead to complete separation.\n",
        "if 1 in arbs_event_crosstab.index:\n",
        "    if arbs_event_crosstab.loc[1, 0] == 0 and arbs_event_crosstab.loc[1, 1] > 0:\n",
        "        print(\"\\nComplete separation likely exists for 'on_arbs' (all events in category 1).\")\n",
        "    elif arbs_event_crosstab.loc[1, 1] == 0 and arbs_event_crosstab.loc[1, 0] > 0:\n",
        "         print(\"\\nComplete separation likely exists for 'on_arbs' (no events in category 1).\")\n",
        "    else:\n",
        "         print(\"\\nComplete separation does not appear to exist for 'on_arbs'.\")\n",
        "elif 0 in arbs_event_crosstab.index and 1 not in arbs_event_crosstab.index:\n",
        "     print(\"\\n'on_arbs' only has one category present in the data (0), which will cause issues for modeling (complete separation).\")\n",
        "else:\n",
        "    print(\"\\nCould not determine complete separation for 'on_arbs' based on crosstab.\")\n",
        "\n",
        "del correlation_df # Clean up temporary dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e3ab5e1"
      },
      "source": [
        "# Fit Standard Cox Proportional Hazards Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "df7a7eaa"
      },
      "source": [
        "!pip install lifelines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f573f386"
      },
      "source": [
        "from lifelines import CoxPHFitter\n",
        "\n",
        "# Prepare data for Cox model\n",
        "# Need to use the survival_analysis_df which contains time_to_event, event, and the treatment and diagnosis features.\n",
        "# Drop columns not needed for the model (subject_id, date columns, grouped_diagnosis object)\n",
        "cox_data = survival_analysis_df.drop(columns=['subject_id', 'treatment_starttime', 'dialysis_event_date', 'grouped_diagnosis']).copy()\n",
        "\n",
        "# Ensure all predictor columns are numeric\n",
        "for col in cox_data.columns:\n",
        "    if cox_data[col].dtype == 'bool':\n",
        "        cox_data[col] = cox_data[col].astype(int)\n",
        "\n",
        "\n",
        "# Fit the Cox proportional hazards model\n",
        "cph = CoxPHFitter()\n",
        "\n",
        "try:\n",
        "    cph.fit(cox_data, duration_col='time_to_event', event_col='event')\n",
        "\n",
        "    print(\"\\nStandard Cox Proportional Hazards Model Summary:\")\n",
        "    cph.print_summary()\n",
        "\n",
        "    # Evaluate the fitted model using concordance index\n",
        "    c_index_cox = cph.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Standard Cox model: {c_index_cox:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during standard Cox model fitting: {e}\")\n",
        "    print(\"The model may not have converged due to issues like complete separation or multicollinearity.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2434137d"
      },
      "source": [
        "# Explore alternative survival models\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lifelines import CoxPHFitter\n",
        "\n",
        "# Prepare data for Cox model\n",
        "# Need to use the survival_analysis_df which contains time_to_event, event, and the treatment and diagnosis features.\n",
        "# Drop columns not needed for the model (subject_id, date columns, grouped_diagnosis object)\n",
        "cox_data = survival_analysis_df.drop(columns=['subject_id', 'treatment_starttime', 'dialysis_event_date', 'grouped_diagnosis']).copy()\n",
        "\n",
        "# Ensure all predictor columns are numeric\n",
        "for col in cox_data.columns:\n",
        "    if cox_data[col].dtype == 'bool':\n",
        "        cox_data[col] = cox_data[col].astype(int)\n",
        "\n",
        "\n",
        "# Fit a Penalized Cox Proportional Hazards Model (with L2 regularization)\n",
        "cph_penalized = CoxPHFitter(penalizer=0.1)\n",
        "\n",
        "try:\n",
        "    cph_penalized.fit(cox_data, duration_col='time_to_event', event_col='event')\n",
        "\n",
        "    print(\"\\nPenalized Cox Proportional Hazards Model Summary (L2 Regularization):\")\n",
        "    cph_penalized.print_summary()\n",
        "\n",
        "    # Evaluate the fitted model using concordance index\n",
        "    c_index_penalized = cph_penalized.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Penalized Cox model: {c_index_penalized:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during penalized Cox model fitting: {e}\")"
      ],
      "metadata": {
        "id": "4tyIjojEqSWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00d8af5b"
      },
      "source": [
        "## Feature Engineering: Interaction Terms and Number of Treatments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "256e9082"
      },
      "source": [
        "# 1. Create a feature for the number of CKD guideline-recommended treatment categories each subject is on.\n",
        "# Sum the binary columns for each treatment category\n",
        "survival_analysis_df['num_ckd_grt_categories'] = survival_analysis_df[[\n",
        "    'on_sglt2_inhibitor', 'on_ace_inhibitor', 'on_arbs', 'on_statin'\n",
        "]].sum(axis=1)\n",
        "\n",
        "print(\"\\nAdded 'num_ckd_grt_categories' feature:\")\n",
        "display(survival_analysis_df[['subject_id', 'num_ckd_grt_categories']].head())\n",
        "\n",
        "print(\"\\nValue counts for 'num_ckd_grt_categories':\")\n",
        "display(survival_analysis_df['num_ckd_grt_categories'].value_counts())\n",
        "\n",
        "\n",
        "# 2. Create interaction terms between grouped CKD diagnosis categories and individual treatment categories.\n",
        "survival_analysis_df = pd.get_dummies(survival_analysis_df, columns=['grouped_diagnosis'], drop_first=False)\n",
        "\n",
        "\n",
        "# Create interaction terms by multiplying the one-hot encoded diagnosis columns with the treatment columns\n",
        "diagnosis_categories = [col for col in survival_analysis_df.columns if col.startswith('grouped_diagnosis_')]\n",
        "treatment_categories = ['on_sglt2_inhibitor', 'on_ace_inhibitor', 'on_arbs', 'on_statin']\n",
        "\n",
        "for diag_col in diagnosis_categories:\n",
        "    for treat_col in treatment_categories:\n",
        "        interaction_col_name = f'{diag_col}_x_{treat_col}'\n",
        "        survival_analysis_df[interaction_col_name] = survival_analysis_df[diag_col] * survival_analysis_df[treat_col]\n",
        "\n",
        "print(\"\\nAdded interaction terms to survival_analysis_df:\")\n",
        "display(survival_analysis_df.head())\n",
        "\n",
        "print(\"\\nInfo of survival_analysis_df after adding new features:\")\n",
        "survival_analysis_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc96b1a0"
      },
      "source": [
        "## Fit Penalized Cox Model with Interaction Terms and Number of Treatments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59e83e78"
      },
      "source": [
        "from lifelines import CoxPHFitter\n",
        "\n",
        "# Prepare data for the penalized Cox model with new features\n",
        "# Drop columns not needed for the model (subject_id, date columns)\n",
        "# The original 'grouped_diagnosis' column has already been one-hot encoded and dropped in the previous step.\n",
        "cox_data_interactions = survival_analysis_df.drop(columns=['subject_id', 'treatment_starttime', 'dialysis_event_date']).copy()\n",
        "\n",
        "# Ensure all predictor columns are numeric\n",
        "for col in cox_data_interactions.columns:\n",
        "    if cox_data_interactions[col].dtype == 'bool':\n",
        "        cox_data_interactions[col] = cox_data_interactions[col].astype(int)\n",
        "\n",
        "\n",
        "# Fit a Penalized Cox Proportional Hazards Model (with L2 regularization)\n",
        "cph_penalized_interactions = CoxPHFitter(penalizer=0.1)\n",
        "\n",
        "try:\n",
        "    cph_penalized_interactions.fit(cox_data_interactions, duration_col='time_to_event', event_col='event')\n",
        "\n",
        "    print(\"\\nPenalized Cox Proportional Hazards Model Summary (with Interaction Terms and Number of Treatments):\")\n",
        "    cph_penalized_interactions.print_summary()\n",
        "\n",
        "    # Evaluate the fitted model using concordance index\n",
        "    c_index_penalized_interactions = cph_penalized_interactions.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Penalized Cox model (with Interactions): {c_index_penalized_interactions:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during penalized Cox model fitting with interaction terms: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39906afd"
      },
      "source": [
        "## Examine Columns with Low Variance and Potential Complete Separation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3996f0dd"
      },
      "source": [
        "# Examine the variance of the identified low-variance columns\n",
        "low_variance_cols = [\n",
        "    'grouped_diagnosis_Hypertensive CKD (Stages 5-End Stage)_x_on_sglt2_inhibitor'\n",
        "]\n",
        "\n",
        "print(\"Variance of identified low-variance columns:\")\n",
        "display(survival_analysis_df[low_variance_cols].var())\n",
        "\n",
        "# Examine the relationship with the event column for complete separation\n",
        "print(\"\\nRelationship between low-variance columns and 'event' (for complete separation check):\")\n",
        "\n",
        "for col in low_variance_cols:\n",
        "    print(f\"\\nCrosstab for '{col}' and 'event':\")\n",
        "    crosstab_result = pd.crosstab(survival_analysis_df[col], survival_analysis_df['event'])\n",
        "    display(crosstab_result)\n",
        "\n",
        "    # Check for complete separation: if all events (event=1) occur in only one category of the feature\n",
        "    if 1 in crosstab_result.index:\n",
        "        if crosstab_result.loc[1, 0] == 0 and crosstab_result.loc[1, 1] > 0:\n",
        "            print(f\"Complete separation likely exists for '{col}' (all events in category 1).\")\n",
        "        elif crosstab_result.loc[1, 1] == 0 and crosstab_result.loc[1, 0] > 0:\n",
        "             print(f\"Complete separation likely exists for '{col}' (no events in category 1).\")\n",
        "        else:\n",
        "             print(f\"Complete separation does not appear to exist for '{col}'.\")\n",
        "    elif 0 in crosstab_result.index and 1 not in crosstab_result.index:\n",
        "         print(f\"'{col}' only has one category present in the data (0), which will cause issues for modeling (complete separation).\")\n",
        "    else:\n",
        "        print(f\"Could not determine complete separation for '{col}' based on crosstab.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78206d85"
      },
      "source": [
        "## Remove Problematic Features and Refit Penalized Cox Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d86144d"
      },
      "source": [
        "# Identify columns to drop due to complete separation\n",
        "columns_to_drop = [\n",
        "    'grouped_diagnosis_Hypertensive CKD (Stages 5-End Stage)_x_on_sglt2_inhibitor'\n",
        "]\n",
        "\n",
        "# Drop the problematic columns from the dataframe used for modeling\n",
        "cox_data_interactions_cleaned = cox_data_interactions.drop(columns=columns_to_drop).copy()\n",
        "\n",
        "print(\"Dropped problematic columns due to complete separation.\")\n",
        "print(f\"Remaining columns for modeling: {cox_data_interactions_cleaned.columns.tolist()}\")\n",
        "\n",
        "\n",
        "# Fit a Penalized Cox Proportional Hazards Model (with L2 regularization) using the cleaned data.\n",
        "cph_penalized_cleaned = CoxPHFitter(penalizer=0.1)\n",
        "\n",
        "try:\n",
        "    cph_penalized_cleaned.fit(cox_data_interactions_cleaned, duration_col='time_to_event', event_col='event')\n",
        "\n",
        "    print(\"\\nPenalized Cox Proportional Hazards Model Summary (Cleaned Data):\")\n",
        "    cph_penalized_cleaned.print_summary()\n",
        "\n",
        "    # Evaluate the fitted model using concordance index\n",
        "    c_index_penalized_cleaned = cph_penalized_cleaned.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Penalized Cox model (Cleaned Data): {c_index_penalized_cleaned:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during penalized Cox model fitting with cleaned data: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e37def74"
      },
      "source": [
        "# Penalized Cox post cleaning\n",
        "exclude previously identified problematic interaction terms ( 'grouped_diagnosis_Hypertensive CKD (Stages 5-End Stage)_x_on_sglt2_inhibitor', 'grouped_diagnosis_Stage 1 CKD_x_on_sglt2_inhibitor', 'grouped_diagnosis_Stage 5 CKD_x_on_sglt2_inhibitor')\n",
        "\n",
        "identify & remove any columns with zero or extremely low variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e321d959"
      },
      "source": [
        "## Consolidate and Prepare Data for Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a26daeeb"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Define a list of columns to be excluded from survival_analysis_df\n",
        "columns_to_drop_modeling = [\n",
        "    'subject_id',\n",
        "    'treatment_starttime',\n",
        "    'dialysis_event_date',\n",
        "    'grouped_diagnosis_Hypertensive CKD (Stages 5-End Stage)_x_on_sglt2_inhibitor',\n",
        "    'grouped_diagnosis_Stage 5 CKD_x_on_sglt2_inhibitor'\n",
        "]\n",
        "\n",
        "# 2. Create cox_data_full by dropping these identified columns from survival_analysis_df.\n",
        "cox_data_full = survival_analysis_df.drop(columns=columns_to_drop_modeling).copy()\n",
        "\n",
        "# 3. Iterate through all columns in the new cox_data_full DataFrame. If a column's data type is boolean, convert it to an integer type (0 or 1).\n",
        "for col in cox_data_full.columns:\n",
        "    if cox_data_full[col].dtype == 'bool':\n",
        "        cox_data_full[col] = cox_data_full[col].astype(int)\n",
        "\n",
        "print(\"Recreated cox_data_full DataFrame:\")\n",
        "# 4. Display the first few rows of the prepared cox_data_full DataFrame using display().\n",
        "display(cox_data_full.head())\n",
        "\n",
        "print(\"\\nInfo of the recreated cox_data_full DataFrame:\")\n",
        "# 5. Print the .info() of the cox_data_full DataFrame to show its structure and data types.\n",
        "cox_data_full.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "728e08ac"
      },
      "source": [
        "import numpy as np\n",
        "from lifelines import CoxPHFitter\n",
        "\n",
        "# Identify predictor columns (excluding 'time_to_event' and 'event')\n",
        "predictor_cols = [col for col in cox_data_full.columns if col not in ['time_to_event', 'event']]\n",
        "\n",
        "problematic_cols = []\n",
        "print(\"\\n--- Checking for low variance and complete separation ---\")\n",
        "\n",
        "# Iterate through predictor columns to check variance and complete separation\n",
        "for col in predictor_cols:\n",
        "    col_variance = cox_data_full[col].var()\n",
        "    if col_variance < 1e-6:\n",
        "        print(f\"\\nColumn '{col}' has very low variance: {col_variance:.2e}\")\n",
        "\n",
        "        # Perform crosstab with 'event' to confirm complete separation\n",
        "        crosstab_result = pd.crosstab(cox_data_full[col], cox_data_full['event'])\n",
        "        print(f\"Crosstab for '{col}' and 'event':\")\n",
        "        display(crosstab_result)\n",
        "\n",
        "        # Check for complete separation logic\n",
        "        if 1 in crosstab_result.index:\n",
        "            if (crosstab_result.loc[1, 0] == 0 and crosstab_result.loc[1, 1] > 0) or \\\n",
        "               (crosstab_result.loc[1, 1] == 0 and crosstab_result.loc[1, 0] > 0):\n",
        "                print(f\"Complete separation confirmed for '{col}'. Adding to problematic list.\")\n",
        "                problematic_cols.append(col)\n",
        "            else:\n",
        "                print(f\"Low variance for '{col}' but complete separation is not evident from crosstab.\")\n",
        "        elif crosstab_result.shape[0] == 1:\n",
        "             print(f\"Column '{col}' has only one unique value. Adding to problematic list.\")\n",
        "             problematic_cols.append(col)\n",
        "        else:\n",
        "             print(f\"Low variance for '{col}' but complete separation check inconclusive.\")\n",
        "\n",
        "# Remove duplicate columns from problematic_cols if any\n",
        "problematic_cols = list(set(problematic_cols))\n",
        "\n",
        "if problematic_cols:\n",
        "    print(f\"\\nIdentified problematic columns to drop: {problematic_cols}\")\n",
        "    cox_data_final = cox_data_full.drop(columns=problematic_cols).copy()\n",
        "else:\n",
        "    print(\"\\nNo additional problematic columns identified based on low variance/complete separation.\")\n",
        "    cox_data_final = cox_data_full.copy()\n",
        "\n",
        "print(\"\\nRemaining columns for modeling (cox_data_final):\")\n",
        "print(cox_data_final.columns.tolist())\n",
        "\n",
        "# Add a small positive value to any zero durations if they exist, as required by lifelines\n",
        "zero_duration_mask = cox_data_final['time_to_event'] <= 0\n",
        "if zero_duration_mask.any():\n",
        "    epsilon = np.finfo(float).eps\n",
        "    cox_data_final.loc[zero_duration_mask, 'time_to_event'] = epsilon\n",
        "    print(f\"Added {epsilon} to {zero_duration_mask.sum()} zero or negative durations in 'time_to_event'.\")\n",
        "\n",
        "\n",
        "# Fit a Penalized Cox Proportional Hazards Model (with L2 regularization) using the cleaned data\n",
        "cph_final = CoxPHFitter(penalizer=0.1) # Using penalizer=0.1 as it previously showed good performance\n",
        "\n",
        "try:\n",
        "    cph_final.fit(cox_data_final, duration_col='time_to_event', event_col='event')\n",
        "\n",
        "    print(\"\\n--- Penalized Cox Proportional Hazards Model Summary (Final Cleaned Data) ---\")\n",
        "    cph_final.print_summary()\n",
        "\n",
        "    # Evaluate the fitted model using concordance index\n",
        "    c_index_final = cph_final.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Penalized Cox model (Final Cleaned Data): {c_index_final:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during penalized Cox model fitting with final cleaned data: {e}\")\n",
        "    c_index_final = None\n",
        "\n",
        "# Summarize the process and results\n",
        "print(\"\\n--- Summary of Process and Results ---\")\n",
        "print(\"1. The `cox_data_full` DataFrame was prepared by excluding identifier/date columns and previously identified problematic interaction terms.\")\n",
        "print(\"2. An iterative check for extremely low variance columns was performed, and complete separation was confirmed for these using crosstabs against the 'event' column.\")\n",
        "print(f\"3. The following columns were identified and removed due to low variance/complete separation: {problematic_cols if problematic_cols else 'None'}.\")\n",
        "print(\"4. A small positive epsilon value was added to any zero or negative 'time_to_event' durations to ensure compatibility with `lifelines` models.\")\n",
        "print(f\"5. A penalized CoxPHFitter model (L2 regularization, penalizer=0.1) was fitted using the `cox_data_final` dataset.\")\n",
        "if c_index_final is not None:\n",
        "    print(f\"6. The model converged successfully with a Concordance Index of {c_index_final:.4f}, indicating good predictive performance.\")\n",
        "    print(\"7. The model summary provides insights into the significance and hazard ratios of the remaining predictors. Significance of predictors can now be interpreted more reliably as convergence issues related to separation have been addressed.\")\n",
        "else:\n",
        "    print(\"6. The model did not converge successfully even after cleaning, suggesting further investigation into multicollinearity or data structure may be needed.\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed9c6dad"
      },
      "source": [
        "# Simpler penalized cox model (main effects only)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "809c003b"
      },
      "source": [
        "from lifelines import CoxPHFitter\n",
        "\n",
        "# 1. Create a new DataFrame cox_data_main_effects by selecting the relevant columns\n",
        "# Main effect predictor columns: binary treatment features and one-hot encoded diagnosis features\n",
        "main_effect_cols = [\n",
        "    'time_to_event',\n",
        "    'event',\n",
        "    'on_sglt2_inhibitor',\n",
        "    'on_ace_inhibitor',\n",
        "    'on_arbs',\n",
        "    'on_statin'\n",
        "]\n",
        "\n",
        "# Add the one-hot encoded grouped diagnosis columns\n",
        "diagnosis_cols_one_hot = [col for col in survival_analysis_df.columns if col.startswith('grouped_diagnosis_')]\n",
        "main_effect_cols.extend(diagnosis_cols_one_hot)\n",
        "\n",
        "# Select these columns from survival_analysis_df\n",
        "cox_data_main_effects = survival_analysis_df[main_effect_cols].copy()\n",
        "\n",
        "# 2. Ensure all selected predictor columns are numeric (convert boolean to int if necessary)\n",
        "for col in cox_data_main_effects.columns:\n",
        "    if cox_data_main_effects[col].dtype == 'bool':\n",
        "        cox_data_main_effects[col] = cox_data_main_effects[col].astype(int)\n",
        "\n",
        "print(\"Prepared data for main effects Cox model:\")\n",
        "display(cox_data_main_effects.head())\n",
        "cox_data_main_effects.info()\n",
        "\n",
        "# 3. Instantiate a CoxPHFitter object with a chosen penalizer value\n",
        "# Based on the previous experimentation, penalizer=0.1 performed well.\n",
        "chosen_penalizer = 0.1\n",
        "cph_main_effects = CoxPHFitter(penalizer=chosen_penalizer)\n",
        "\n",
        "# 4. Fit the CoxPHFitter model\n",
        "try:\n",
        "    cph_main_effects.fit(cox_data_main_effects, duration_col='time_to_event', event_col='event')\n",
        "\n",
        "    # 5. Print the summary of the fitted model\n",
        "    print(f\"\\nPenalized Cox Proportional Hazards Model Summary (Main Effects Only, Penalizer={chosen_penalizer}):\")\n",
        "    cph_main_effects.print_summary()\n",
        "\n",
        "    # 6. Calculate and print the concordance index\n",
        "    c_index_main_effects = cph_main_effects.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Penalized Cox model (Main Effects Only): {c_index_main_effects:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during main effects Cox model fitting: {e}\")\n",
        "\n",
        "# 7. Delete the cox_data_main_effects DataFrame\n",
        "del cox_data_main_effects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkOmCndl3Moq"
      },
      "source": [
        "from lifelines import CoxPHFitter\n",
        "import numpy as np\n",
        "\n",
        "# Re-define the main effect predictor columns explicitly, excluding interaction terms and num_ckd_grt_categories\n",
        "main_effect_cols = [\n",
        "    'time_to_event',\n",
        "    'event',\n",
        "    'on_sglt2_inhibitor',\n",
        "    'on_ace_inhibitor',\n",
        "    'on_arbs',\n",
        "    'on_statin'\n",
        "]\n",
        "\n",
        "# Add the one-hot encoded grouped diagnosis columns.\n",
        "diagnosis_cols_one_hot = [col for col in survival_analysis_df.columns\n",
        "                          if col.startswith('grouped_diagnosis_') and '_x_' not in col]\n",
        "\n",
        "main_effect_cols.extend(diagnosis_cols_one_hot)\n",
        "\n",
        "# Select these columns from survival_analysis_df\n",
        "cox_data_main_effects = survival_analysis_df[main_effect_cols].copy()\n",
        "\n",
        "# Ensure all selected predictor columns are numeric (convert boolean to int if necessary)\n",
        "for col in cox_data_main_effects.columns:\n",
        "    if cox_data_main_effects[col].dtype == 'bool':\n",
        "        cox_data_main_effects[col] = cox_data_main_effects[col].astype(int)\n",
        "\n",
        "print(\"Prepared data for main effects Cox model (re-checked columns):\")\n",
        "display(cox_data_main_effects.head())\n",
        "cox_data_main_effects.info()\n",
        "\n",
        "\n",
        "predictor_cols_for_check = [col for col in cox_data_main_effects.columns if col not in ['time_to_event', 'event']]\n",
        "\n",
        "problematic_main_effect_cols = []\n",
        "print(\"\\n--- Checking for low variance and complete separation in main effects model ---\")\n",
        "\n",
        "for col in predictor_cols_for_check:\n",
        "    col_variance = cox_data_main_effects[col].var()\n",
        "    if col_variance < 1e-6:\n",
        "        print(f\"\\nColumn '{col}' has very low variance: {col_variance:.2e}\")\n",
        "\n",
        "        # Perform crosstab with 'event' to confirm complete separation\n",
        "        crosstab_result = pd.crosstab(cox_data_main_effects[col], cox_data_main_effects['event'])\n",
        "        print(f\"Crosstab for '{col}' and 'event':\")\n",
        "        display(crosstab_result)\n",
        "\n",
        "        # Check for complete separation logic\n",
        "        if 1 in crosstab_result.index:\n",
        "            if (crosstab_result.loc[1, 0] == 0 and crosstab_result.loc[1, 1] > 0) or \\\n",
        "               (crosstab_result.loc[1, 1] == 0 and crosstab_result.loc[1, 0] > 0):\n",
        "                print(f\"Complete separation confirmed for '{col}'. Adding to problematic list.\")\n",
        "                problematic_main_effect_cols.append(col)\n",
        "            elif crosstab_result.shape[0] == 1:\n",
        "                 print(f\"Column '{col}' has only one unique value. Adding to problematic list.\")\n",
        "                 problematic_main_effect_cols.append(col)\n",
        "            else:\n",
        "                print(f\"Low variance for '{col}' but complete separation is not evident from crosstab.\")\n",
        "        elif crosstab_result.shape[0] == 1:\n",
        "             print(f\"Column '{col}' has only one unique value. Adding to problematic list.\")\n",
        "             problematic_main_effect_cols.append(col)\n",
        "        else:\n",
        "             print(f\"Low variance for '{col}' but complete separation check inconclusive.\")\n",
        "\n",
        "# Remove duplicate columns from problematic_main_effect_cols if any\n",
        "problematic_main_effect_cols = list(set(problematic_main_effect_cols))\n",
        "\n",
        "if problematic_main_effect_cols:\n",
        "    print(f\"\\nIdentified problematic columns to drop for main effects model: {problematic_main_effect_cols}\")\n",
        "    cox_data_main_effects_cleaned = cox_data_main_effects.drop(columns=problematic_main_effect_cols).copy()\n",
        "else:\n",
        "    print(\"\\nNo additional problematic columns identified based on low variance/complete separation for main effects model.\")\n",
        "    cox_data_main_effects_cleaned = cox_data_main_effects.copy()\n",
        "\n",
        "print(\"\\nRemaining columns for main effects modeling (cox_data_main_effects_cleaned):\")\n",
        "print(cox_data_main_effects_cleaned.columns.tolist())\n",
        "\n",
        "# Add a small positive value to any zero durations if they exist, as required by lifelines\n",
        "zero_duration_mask_main_effects = cox_data_main_effects_cleaned['time_to_event'] <= 0\n",
        "if zero_duration_mask_main_effects.any():\n",
        "    epsilon = np.finfo(float).eps\n",
        "    cox_data_main_effects_cleaned.loc[zero_duration_mask_main_effects, 'time_to_event'] = epsilon\n",
        "    print(f\"Added {epsilon} to {zero_duration_mask_main_effects.sum()} zero or negative durations in 'time_to_event' for main effects cleaned data.\")\n",
        "\n",
        "\n",
        "# Instantiate a CoxPHFitter object with a chosen penalizer value\n",
        "# Based on the previous experimentation, penalizer=0.1 performed well.\n",
        "chosen_penalizer = 0.1\n",
        "cph_main_effects = CoxPHFitter(penalizer=chosen_penalizer)\n",
        "\n",
        "# Fit the CoxPHFitter model\n",
        "try:\n",
        "    cph_main_effects.fit(cox_data_main_effects_cleaned, duration_col='time_to_event', event_col='event')\n",
        "\n",
        "    # 5. Print the summary of the fitted model\n",
        "    print(f\"\\nPenalized Cox Proportional Hazards Model Summary (Main Effects Only, Penalizer={chosen_penalizer}):\")\n",
        "    cph_main_effects.print_summary()\n",
        "\n",
        "    # 6. Calculate and print the concordance index\n",
        "    c_index_main_effects = cph_main_effects.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Penalized Cox model (Main Effects Only): {c_index_main_effects:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during main effects Cox model fitting: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaNT9-fn5aJI"
      },
      "source": [
        "# Interpretation of the Main Effects Penalized Cox Model Summary:\n",
        "\n",
        "print(\"\\nInterpretation of the Penalized Cox Model Summary (Main Effects Only):\")\n",
        "\n",
        "print(\"\\nOverall Model Fit:\")\n",
        "if hasattr(cph_main_effects, 'log_likelihood_') and hasattr(cph_main_effects, 'summary'):\n",
        "    print(f\"* Partial Log-Likelihood: {cph_main_effects.log_likelihood_}\")\n",
        "\n",
        "    try:\n",
        "        ll_ratio_test = cph_main_effects.summary.loc['log-likelihood ratio test', 'coef']\n",
        "        ll_ratio_p_value = cph_main_effects.summary.loc['log-likelihood ratio test', 'p']\n",
        "        ll_ratio_dof = cph_main_effects.summary.loc['log-likelihood ratio test', 'df']\n",
        "\n",
        "        print(f\"* Log-likelihood ratio test: {ll_ratio_test:.2f} on {ll_ratio_dof} df, p-value = {ll_ratio_p_value:.3f}.\")\n",
        "        if ll_ratio_p_value < 0.005:\n",
        "            print(\"This highly significant result indicates that the model with these predictors fits the data significantly better than a model without predictors.\")\n",
        "        else:\n",
        "            print(\"This result suggests that the model with these predictors does not significantly improve the fit compared to a model without predictors.\")\n",
        "\n",
        "    except KeyError:\n",
        "        print(\"\\nCould not find 'log-likelihood ratio test' in the summary index. The summary structure might have changed.\")\n",
        "        print(\"\\nFull model summary:\")\n",
        "        cph_main_effects.print_summary()\n",
        "else:\n",
        "    print(\"Model did not converge successfully, cannot provide overall model fit statistics.\")\n",
        "\n",
        "\n",
        "print(\"\\nIndividual Predictor Interpretation (Hazard Ratios and Significance):\")\n",
        "print(\"Hazard ratios exp(coef) less than 1 indicate a decreased hazard (lower risk of dialysis).\")\n",
        "print(\"Hazard ratios exp(coef) greater than 1 indicate an increased hazard (higher risk of dialysis).\")\n",
        "\n",
        "# Ensure summary is available before proceeding\n",
        "if hasattr(cph_main_effects, 'summary'):\n",
        "    summary_df = cph_main_effects.summary.copy()\n",
        "    # Remove the overall test rows for individual predictor interpretation\n",
        "    rows_to_remove = ['log-likelihood ratio test', 'Concordance', 'Partial AIC', 'log-likelihood ratio test (p=?)'] # Added another common index\n",
        "    summary_df = summary_df[~summary_df.index.isin(rows_to_remove)]\n",
        "\n",
        "    alpha = 0.05\n",
        "\n",
        "    for index, row in summary_df.iterrows():\n",
        "        # Check if the row has expected columns before accessing\n",
        "        if 'exp(coef)' in row and 'p' in row and 'exp(coef) lower 95%' in row and 'exp(coef) upper 95%' in row:\n",
        "            covariate = index\n",
        "            hazard_ratio = row['exp(coef)']\n",
        "            p_value = row['p']\n",
        "            ci_lower = row['exp(coef) lower 95%']\n",
        "            ci_upper = row['exp(coef) upper 95%']\n",
        "\n",
        "            significance = \"Statistically significant\" if p_value < alpha else \"Not statistically significant\"\n",
        "            effect = \"decreased\" if hazard_ratio < 1 else \"increased\" if hazard_ratio > 1 else \"no significant change in\"\n",
        "\n",
        "            print(f\"* {covariate}: Hazard Ratio = {hazard_ratio:.2f} (95% CI: {ci_lower:.2f} - {ci_upper:.2f}), p={p_value:.3f}. {significance}.\")\n",
        "            if significance == \"Statistically significant\":\n",
        "                print(f\"  Being in this category is associated with a significantly {effect} hazard of going on dialysis compared to the reference group (or other categories in a one-hot encoded set).\")\n",
        "            else:\n",
        "                print(f\"  This predictor is not statistically significantly associated with the hazard of going on dialysis in this model.\")\n",
        "        else:\n",
        "            print(f\"* Warning: Missing expected columns in summary row for {index}. Skipping interpretation.\")\n",
        "else:\n",
        "    print(\"\\nModel did not converge successfully, cannot interpret individual predictors.\")\n",
        "\n",
        "print(\"\\nModel Performance:\")\n",
        "if hasattr(cph_main_effects, 'concordance_index_'):\n",
        "    print(f\"* Concordance Index: {cph_main_effects.concordance_index_:.4f}. A value of {cph_main_effects.concordance_index_:.4f} suggests reasonably good discriminatory power.\")\n",
        "else:\n",
        "    print(\"* Concordance Index not available as model did not converge.\")\n",
        "\n",
        "print(\"\\nNote on Diagnosis Interpretation:\")\n",
        "print(\"Since one-hot encoding was performed without dropping the first category, there is no single explicit reference diagnosis group.\")\n",
        "print(\"The coefficients for the grouped diagnosis categories represent the difference in the log-hazard compared to a baseline where all diagnosis dummy variables are zero (which doesn't correspond to a real group).\")\n",
        "print(\"For easier interpretation, you could re-run the one-hot encoding specifying one category as the reference by using `drop_first=True`.\")\n",
        "\n",
        "# Final check on the subtask completion\n",
        "print(\"\\nSubtask 'Fit a penalized Cox model including only the main effects...' is completed.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "951d09ba"
      },
      "source": [
        "# Lasso & Elastic Net (Simplified)\n",
        "Lasso (L1, `l1_ratio=1.0`, `penalizer=0.1`) and Elastic Net (`l1_ratio=0.5`, `penalizer=0.1`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da758ac0"
      },
      "source": [
        "## Prepare Data for Simplified Regularized Cox Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4962c55d"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Create a new DataFrame by selecting the specified columns\n",
        "simplified_cox_data = survival_analysis_df[[\n",
        "    'time_to_event',\n",
        "    'event',\n",
        "    'on_sglt2_inhibitor',\n",
        "    'on_ace_inhibitor',\n",
        "    'on_arbs',\n",
        "    'on_statin'\n",
        "]].copy()\n",
        "\n",
        "# 2. Iterate through all columns in the simplified_cox_data DataFrame and convert boolean to int\n",
        "for col in simplified_cox_data.columns:\n",
        "    if simplified_cox_data[col].dtype == 'bool':\n",
        "        simplified_cox_data[col] = simplified_cox_data[col].astype(int)\n",
        "\n",
        "# 3. Check for any 'time_to_event' values that are less than or equal to zero and adjust\n",
        "zero_duration_mask = simplified_cox_data['time_to_event'] <= 0\n",
        "if zero_duration_mask.any():\n",
        "    epsilon = 1e-6 # A very small positive number\n",
        "    simplified_cox_data.loc[zero_duration_mask, 'time_to_event'] = epsilon\n",
        "    print(f\"Added {epsilon} to {zero_duration_mask.sum()} zero or negative durations in 'time_to_event'.\")\n",
        "\n",
        "print(\"Prepared simplified_cox_data DataFrame:\")\n",
        "# 4. Display the head of the simplified_cox_data DataFrame.\n",
        "display(simplified_cox_data.head())\n",
        "\n",
        "print(\"\\nInfo of the simplified_cox_data DataFrame:\")\n",
        "# 5. Print the .info() of the simplified_cox_data DataFrame.\n",
        "simplified_cox_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ac1311f"
      },
      "source": [
        "from lifelines import CoxPHFitter\n",
        "\n",
        "# 1. Fit a Lasso (L1) penalized Cox model\n",
        "# Lasso regularization (L1_ratio = 1.0) for feature selection\n",
        "print(\"\\n--- Fitting Lasso (L1) Penalized Cox Model ---\")\n",
        "cph_lasso = CoxPHFitter(penalizer=0.1, l1_ratio=1.0)\n",
        "try:\n",
        "    cph_lasso.fit(simplified_cox_data, duration_col='time_to_event', event_col='event')\n",
        "\n",
        "    print(\"\\nLasso Penalized Cox Model Summary:\")\n",
        "    cph_lasso.print_summary()\n",
        "\n",
        "    # Evaluate the fitted model using concordance index\n",
        "    c_index_lasso = cph_lasso.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Lasso Cox model: {c_index_lasso:.4f}\")\n",
        "\n",
        "    print(\"\\nCoefficients of Lasso Cox Model:\")\n",
        "    display(cph_lasso.params_)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Lasso Cox model fitting: {e}\")\n",
        "\n",
        "\n",
        "# 2. Fit an Elastic Net penalized Cox model\n",
        "# Elastic Net regularization (L1_ratio = 0.5 for a mix of L1 and L2) for balanced regularization\n",
        "print(\"\\n\\n--- Fitting Elastic Net Penalized Cox Model ---\")\n",
        "cph_elastic_net = CoxPHFitter(penalizer=0.1, l1_ratio=0.5)\n",
        "try:\n",
        "    cph_elastic_net.fit(simplified_cox_data, duration_col='time_to_event', event_col='event')\n",
        "\n",
        "    print(\"\\nElastic Net Penalized Cox Model Summary:\")\n",
        "    cph_elastic_net.print_summary()\n",
        "\n",
        "    # Evaluate the fitted model using concordance index\n",
        "    c_index_elastic_net = cph_elastic_net.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Elastic Net Cox model: {c_index_elastic_net:.4f}\")\n",
        "\n",
        "    print(\"\\nCoefficients of Elastic Net Cox Model:\")\n",
        "    display(cph_elastic_net.params_)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Elastic Net Cox model fitting: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "530ff64f"
      },
      "source": [
        "## Fit Lasso (L1) Penalized Cox Model (Simplified)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "199ac7ee"
      },
      "source": [
        "from lifelines import CoxPHFitter\n",
        "\n",
        "# 1. Instantiate a CoxPHFitter object named cph_lasso\n",
        "cph_lasso = CoxPHFitter(penalizer=0.1, l1_ratio=1.0) # l1_ratio=1.0 for pure Lasso regularization\n",
        "\n",
        "# 2. Fit the cph_lasso model to the prepared simplified data\n",
        "try:\n",
        "    cph_lasso.fit(simplified_cox_data, duration_col='time_to_event', event_col='event')\n",
        "\n",
        "    # 3. Print the summary of the fitted cph_lasso model\n",
        "    print(\"\\nLasso Penalized Cox Model Summary:\")\n",
        "    cph_lasso.print_summary()\n",
        "\n",
        "    # 4. Calculate and print the concordance index\n",
        "    c_index_lasso = cph_lasso.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Lasso Cox model: {c_index_lasso:.4f}\")\n",
        "\n",
        "    # 5. Display the coefficients of the cph_lasso model\n",
        "    print(\"\\nCoefficients of Lasso Cox Model:\")\n",
        "    display(cph_lasso.params_)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Lasso Cox model fitting: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b515d171"
      },
      "source": [
        "## Fit Elastic Net Penalized Cox Model (Simplified)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "088e75fe"
      },
      "source": [
        "from lifelines import CoxPHFitter\n",
        "\n",
        "# 1. Instantiate a CoxPHFitter object named cph_elastic_net\n",
        "cph_elastic_net = CoxPHFitter(penalizer=0.1, l1_ratio=0.5) # l1_ratio=0.5 for Elastic Net regularization\n",
        "\n",
        "# 2. Fit the cph_elastic_net model to the prepared simplified data\n",
        "try:\n",
        "    cph_elastic_net.fit(simplified_cox_data, duration_col='time_to_event', event_col='event')\n",
        "\n",
        "    # 3. Print the summary of the fitted cph_elastic_net model\n",
        "    print(\"\\nElastic Net Penalized Cox Model Summary:\")\n",
        "    cph_elastic_net.print_summary()\n",
        "\n",
        "    # 4. Calculate and print the concordance index\n",
        "    c_index_elastic_net = cph_elastic_net.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Elastic Net Cox model: {c_index_elastic_net:.4f}\")\n",
        "\n",
        "    # 5. Display the coefficients of the cph_elastic_net model\n",
        "    print(\"\\nCoefficients of Elastic Net Cox Model:\")\n",
        "    display(cph_elastic_net.params_)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Elastic Net Cox model fitting: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "940a2b92"
      },
      "source": [
        "# Parametric\n",
        "Weibull, Log-Logistic, and Log-Normal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb7731ea"
      },
      "source": [
        "## Prepare Simplified Data for Parametric Models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b728c79f"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Create a new DataFrame called simplified_aft_data by selecting the specified columns\n",
        "simplified_aft_data = survival_analysis_df[[\n",
        "    'time_to_event',\n",
        "    'event',\n",
        "    'on_sglt2_inhibitor',\n",
        "    'on_ace_inhibitor',\n",
        "    'on_arbs',\n",
        "    'on_statin'\n",
        "]].copy()\n",
        "\n",
        "# 2. Iterate through all columns in the simplified_aft_data DataFrame and convert boolean to int\n",
        "for col in simplified_aft_data.columns:\n",
        "    if simplified_aft_data[col].dtype == 'bool':\n",
        "        simplified_aft_data[col] = simplified_aft_data[col].astype(int)\n",
        "\n",
        "# 3. Identify any rows where 'time_to_event' is less than or equal to zero and adjust\n",
        "zero_duration_mask = simplified_aft_data['time_to_event'] <= 0\n",
        "if zero_duration_mask.any():\n",
        "    epsilon = np.finfo(float).eps\n",
        "    simplified_aft_data.loc[zero_duration_mask, 'time_to_event'] = epsilon\n",
        "    print(f\"Added {epsilon} to {zero_duration_mask.sum()} zero or negative durations in 'time_to_event'.\")\n",
        "\n",
        "print(\"Prepared simplified_aft_data DataFrame:\")\n",
        "# 4. Display the first few rows of the simplified_aft_data DataFrame.\n",
        "display(simplified_aft_data.head())\n",
        "\n",
        "print(\"\\nInfo of the simplified_aft_data DataFrame:\")\n",
        "# 5. Print the .info() of the simplified_aft_data DataFrame.\n",
        "simplified_aft_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0303bfd"
      },
      "source": [
        "## Simplified Models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3053978f"
      },
      "source": [
        "from lifelines import WeibullAFTFitter\n",
        "\n",
        "# 1. Instantiate a WeibullAFTFitter object with a small penalizer\n",
        "weibull_aft = WeibullAFTFitter(penalizer=0.01)\n",
        "\n",
        "# 2. Fit the model to the simplified_aft_data DataFrame\n",
        "try:\n",
        "    weibull_aft.fit(simplified_aft_data, duration_col='time_to_event', event_col='event')\n",
        "\n",
        "    # 3. Print the summary of the fitted Weibull AFT model\n",
        "    print(\"\\nWeibull AFT Model Summary (Simplified):\")\n",
        "    weibull_aft.print_summary()\n",
        "\n",
        "    # 4. Calculate and print the concordance index\n",
        "    c_index_weibull = weibull_aft.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Weibull AFT model: {c_index_weibull:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Weibull AFT model fitting: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90af3a7b"
      },
      "source": [
        "from lifelines import LogLogisticAFTFitter\n",
        "\n",
        "# 1. Instantiate a LogLogisticAFTFitter object with a small penalizer\n",
        "loglogistic_aft = LogLogisticAFTFitter(penalizer=0.01)\n",
        "\n",
        "# 2. Fit the model to the simplified_aft_data DataFrame\n",
        "try:\n",
        "    loglogistic_aft.fit(simplified_aft_data, duration_col='time_to_event', event_col='event')\n",
        "\n",
        "    # 3. Print the summary of the fitted Log-Logistic AFT model\n",
        "    print(\"\\nLog-Logistic AFT Model Summary (Simplified):\")\n",
        "    loglogistic_aft.print_summary()\n",
        "\n",
        "    # 4. Calculate and print the concordance index\n",
        "    c_index_loglogistic = loglogistic_aft.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Log-Logistic AFT model: {c_index_loglogistic:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Log-Logistic AFT model fitting: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed243a47"
      },
      "source": [
        "from lifelines import LogNormalAFTFitter\n",
        "\n",
        "# 1. Instantiate a LogNormalAFTFitter object with a small penalizer\n",
        "lognormal_aft = LogNormalAFTFitter(penalizer=0.01)\n",
        "\n",
        "# 2. Fit the model to the simplified_aft_data DataFrame\n",
        "try:\n",
        "    lognormal_aft.fit(simplified_aft_data, duration_col='time_to_event', event_col='event')\n",
        "\n",
        "    # 3. Print the summary of the fitted Log-Normal AFT model\n",
        "    print(\"\\nLog-Normal AFT Model Summary (Simplified):\")\n",
        "    lognormal_aft.print_summary()\n",
        "\n",
        "    # 4. Calculate and print the concordance index\n",
        "    c_index_lognormal = lognormal_aft.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Log-Normal AFT model: {c_index_lognormal:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Log-Normal AFT model fitting: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb132eef"
      },
      "source": [
        "## Comprehensive Models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc86e502"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Create a new DataFrame named comprehensive_aft_data as a copy of survival_analysis_df.\n",
        "comprehensive_aft_data = survival_analysis_df.copy()\n",
        "\n",
        "# 2. Drop the columns 'subject_id', 'treatment_starttime', and 'dialysis_event_date' from comprehensive_aft_data.\n",
        "columns_to_drop_identifiers = [\n",
        "    'subject_id',\n",
        "    'treatment_starttime',\n",
        "    'dialysis_event_date'\n",
        "]\n",
        "comprehensive_aft_data = comprehensive_aft_data.drop(columns=columns_to_drop_identifiers)\n",
        "\n",
        "# 3. Identify the problematic columns previously excluded from the penalized Cox model.\n",
        "# These were identified as causing convergence issues due to complete separation or extremely low variance.\n",
        "problematic_interaction_columns = [\n",
        "    'grouped_diagnosis_Hypertensive CKD (Stages 5-End Stage)_x_on_sglt2_inhibitor',\n",
        "    'grouped_diagnosis_Stage 1 CKD_x_on_sglt2_inhibitor',\n",
        "    'grouped_diagnosis_Stage 5 CKD_x_on_sglt2_inhibitor'\n",
        "]\n",
        "\n",
        "# 4. Remove these problematic columns from comprehensive_aft_data.\n",
        "existing_problematic_cols = [col for col in problematic_interaction_columns if col in comprehensive_aft_data.columns]\n",
        "if existing_problematic_cols:\n",
        "    comprehensive_aft_data = comprehensive_aft_data.drop(columns=existing_problematic_cols)\n",
        "    print(f\"Dropped problematic columns: {existing_problematic_cols}\")\n",
        "else:\n",
        "    print(\"No specified problematic columns found to drop.\")\n",
        "\n",
        "# 5. Iterate through all remaining columns in comprehensive_aft_data and convert any boolean columns to integer type (0 or 1).\n",
        "for col in comprehensive_aft_data.columns:\n",
        "    if comprehensive_aft_data[col].dtype == 'bool':\n",
        "        comprehensive_aft_data[col] = comprehensive_aft_data[col].astype(int)\n",
        "\n",
        "# 6. Check for any 'time_to_event' values that are less than or equal to zero and replace them with a small positive epsilon value.\n",
        "zero_duration_mask = comprehensive_aft_data['time_to_event'] <= 0\n",
        "if zero_duration_mask.any():\n",
        "    epsilon = np.finfo(float).eps\n",
        "    comprehensive_aft_data.loc[zero_duration_mask, 'time_to_event'] = epsilon\n",
        "    print(f\"Added {epsilon} to {zero_duration_mask.sum()} zero or negative durations in 'time_to_event'.\")\n",
        "\n",
        "print(\"\\nPrepared comprehensive_aft_data DataFrame:\")\n",
        "# 7. Display the first few rows of the comprehensive_aft_data DataFrame.\n",
        "display(comprehensive_aft_data.head())\n",
        "\n",
        "print(\"\\nInfo of the comprehensive_aft_data DataFrame:\")\n",
        "# 8. Print the .info() of the comprehensive_aft_data DataFrame.\n",
        "comprehensive_aft_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a5c607a"
      },
      "source": [
        "from lifelines import WeibullAFTFitter\n",
        "\n",
        "# 1. Instantiate a WeibullAFTFitter object\n",
        "weibull_aft_comprehensive = WeibullAFTFitter()\n",
        "\n",
        "# 2. Fit the model to the comprehensive_aft_data DataFrame\n",
        "try:\n",
        "    weibull_aft_comprehensive.fit(comprehensive_aft_data, duration_col='time_to_event', event_col='event')\n",
        "\n",
        "    # 3. Print the summary of the fitted Weibull AFT model\n",
        "    print(\"\\nWeibull AFT Model Summary (Comprehensive):\")\n",
        "    weibull_aft_comprehensive.print_summary()\n",
        "\n",
        "    # 4. Calculate and print the concordance index\n",
        "    c_index_weibull_comprehensive = weibull_aft_comprehensive.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Weibull AFT model (Comprehensive): {c_index_weibull_comprehensive:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Weibull AFT model fitting with comprehensive data: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e35d2bb0"
      },
      "source": [
        "from lifelines import WeibullAFTFitter\n",
        "\n",
        "# Identify additional problematic columns (from the ConvergenceWarning)\n",
        "additional_problematic_cols = [\n",
        "    'grouped_diagnosis_Diabetes with CKD_x_on_sglt2_inhibitor'\n",
        "]\n",
        "\n",
        "# Remove these additional problematic columns from comprehensive_aft_data.\n",
        "existing_additional_problematic_cols = [col for col in additional_problematic_cols if col in comprehensive_aft_data.columns]\n",
        "if existing_additional_problematic_cols:\n",
        "    comprehensive_aft_data = comprehensive_aft_data.drop(columns=existing_additional_problematic_cols).copy()\n",
        "    print(f\"Dropped additional problematic columns: {existing_additional_problematic_cols}\")\n",
        "else:\n",
        "    print(\"No additional specified problematic columns found to drop.\")\n",
        "\n",
        "# Scale the duration vector down to improve convergence as suggested by the error message\n",
        "# Using a copy to avoid modifying the original comprehensive_aft_data directly before dropping all problematic columns\n",
        "comprehensive_aft_data['time_to_event_scaled'] = comprehensive_aft_data['time_to_event'] / 100.0\n",
        "\n",
        "# 1. Instantiate a WeibullAFTFitter object with a small penalizer\n",
        "# A penalizer can help with convergence issues, similar to regularization in Cox models.\n",
        "weibull_aft_comprehensive = WeibullAFTFitter(penalizer=0.01) # Using a small penalizer\n",
        "\n",
        "# 2. Fit the model to the comprehensive_aft_data DataFrame using the scaled duration\n",
        "try:\n",
        "    weibull_aft_comprehensive.fit(comprehensive_aft_data.drop(columns=['time_to_event']),\n",
        "                                  duration_col='time_to_event_scaled',\n",
        "                                  event_col='event')\n",
        "\n",
        "    # 3. Print the summary of the fitted Weibull AFT model\n",
        "    print(\"\\nWeibull AFT Model Summary (Comprehensive, Scaled, Penalized):\")\n",
        "    weibull_aft_comprehensive.print_summary()\n",
        "\n",
        "    # 4. Calculate and print the concordance index\n",
        "    c_index_weibull_comprehensive = weibull_aft_comprehensive.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Weibull AFT model (Comprehensive, Scaled, Penalized): {c_index_weibull_comprehensive:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Weibull AFT model fitting with comprehensive data: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dac8c84"
      },
      "source": [
        "from lifelines import LogLogisticAFTFitter\n",
        "\n",
        "# 1. Instantiate a LogLogisticAFTFitter object with a small penalizer\n",
        "# Using a small penalizer to help with convergence, similar to the Weibull model.\n",
        "loglogistic_aft_comprehensive = LogLogisticAFTFitter(penalizer=0.01)\n",
        "\n",
        "# 2. Fit the model to the comprehensive_aft_data DataFrame using the scaled duration\n",
        "try:\n",
        "    loglogistic_aft_comprehensive.fit(comprehensive_aft_data.drop(columns=['time_to_event']),\n",
        "                                      duration_col='time_to_event_scaled',\n",
        "                                      event_col='event')\n",
        "\n",
        "    # 3. Print the summary of the fitted Log-Logistic AFT model\n",
        "    print(\"\\nLog-Logistic AFT Model Summary (Comprehensive, Scaled, Penalized):\")\n",
        "    loglogistic_aft_comprehensive.print_summary()\n",
        "\n",
        "    # 4. Calculate and print the concordance index\n",
        "    c_index_loglogistic_comprehensive = loglogistic_aft_comprehensive.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Log-Logistic AFT model (Comprehensive, Scaled, Penalized): {c_index_loglogistic_comprehensive:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Log-Logistic AFT model fitting with comprehensive data: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e72d750",
        "collapsed": true
      },
      "source": [
        "from lifelines import LogNormalAFTFitter\n",
        "\n",
        "# 1. Instantiate a LogNormalAFTFitter object with a small penalizer\n",
        "# Using a small penalizer to help with convergence, similar to the previous AFT models.\n",
        "lognormal_aft_comprehensive = LogNormalAFTFitter(penalizer=0.01)\n",
        "\n",
        "# 2. Fit the model to the comprehensive_aft_data DataFrame using the scaled duration\n",
        "try:\n",
        "    lognormal_aft_comprehensive.fit(comprehensive_aft_data.drop(columns=['time_to_event']),\n",
        "                                      duration_col='time_to_event_scaled',\n",
        "                                      event_col='event')\n",
        "\n",
        "    # 3. Print the summary of the fitted Log-Normal AFT model\n",
        "    print(\"\\nLog-Normal AFT Model Summary (Comprehensive, Scaled, Penalized):\")\n",
        "    lognormal_aft_comprehensive.print_summary()\n",
        "\n",
        "    # 4. Calculate and print the concordance index\n",
        "    c_index_lognormal_comprehensive = lognormal_aft_comprehensive.concordance_index_\n",
        "    print(f\"\\nConcordance Index of the Log-Normal AFT model (Comprehensive, Scaled, Penalized): {c_index_lognormal_comprehensive:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Log-Normal AFT model fitting with comprehensive data: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b176611e"
      },
      "source": [
        "### comprehensive Log-Logistic AFT model coef interpretation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba9afb36"
      },
      "source": [
        "print(\"\\nSummary for Log-Logistic AFT Model (Comprehensive, Scaled, Penalized):\\n\")\n",
        "loglogistic_aft_comprehensive.print_summary()\n",
        "\n",
        "print(f\"\\nConcordance Index of the Log-Logistic AFT model (Comprehensive, Scaled, Penalized): {loglogistic_aft_comprehensive.concordance_index_:.4f}\")\n",
        "\n",
        "# Convert the summary to a DataFrame\n",
        "loglogistic_summary_df = loglogistic_aft_comprehensive.summary\n",
        "\n",
        "print(\"\\nInterpretation of Log-Logistic AFT Model Coefficients:\")\n",
        "print(\"For AFT models, exp(coef) > 1 means a longer expected event time (protective effect), and exp(coef) < 1 means a shorter expected event time (increased risk).\")\n",
        "print(\"Statistical significance is generally indicated by p < 0.05.\")\n",
        "\n",
        "# Filter for main effects of GRT categories\n",
        "grt_main_effects = ['on_sglt2_inhibitor', 'on_ace_inhibitor', 'on_arbs', 'on_statin']\n",
        "print(\"\\n--- Main Effects of CKD GRT Categories ---\")\n",
        "for effect in grt_main_effects:\n",
        "    try:\n",
        "        # Corrected indexing: AFT models' coefficients are under 'alpha_'\n",
        "        row = loglogistic_summary_df.loc[('alpha_', effect)]\n",
        "        exp_coef = row['exp(coef)']\n",
        "        p_value = row['p']\n",
        "        if p_value < 0.05:\n",
        "            interpretation = f\"Statistically significant. An {exp_coef:.2f}x {'longer' if exp_coef > 1 else 'shorter'} expected time to dialysis.\"\n",
        "        else:\n",
        "            interpretation = \"Not statistically significant.\"\n",
        "        print(f\"* {effect}: exp(coef) = {exp_coef:.2f}, p = {p_value:.3f}. {interpretation}\")\n",
        "    except KeyError:\n",
        "        print(f\"* {effect}: Not found in summary or potentially dropped (e.g., due to multicollinearity or a common reference category).\")\n",
        "\n",
        "# Interpret 'num_ckd_grt_categories'\n",
        "print(\"\\n--- Main Effect of Number of CKD GRT Categories ---\")\n",
        "try:\n",
        "    # Corrected indexing\n",
        "    row = loglogistic_summary_df.loc[('alpha_', 'num_ckd_grt_categories')]\n",
        "    exp_coef = row['exp(coef)']\n",
        "    p_value = row['p']\n",
        "    if p_value < 0.05:\n",
        "        interpretation = f\"Statistically significant. An {exp_coef:.2f}x {'longer' if exp_coef > 1 else 'shorter'} expected time to dialysis for each additional GRT category.\"\n",
        "    else:\n",
        "        interpretation = \"Not statistically significant.\"\n",
        "    print(f\"* num_ckd_grt_categories: exp(coef) = {exp_coef:.2f}, p = {p_value:.3f}. {interpretation}\")\n",
        "except KeyError:\n",
        "    print(f\"* num_ckd_grt_categories: Not found in summary or potentially dropped.\")\n",
        "\n",
        "# Interpret grouped diagnosis categories\n",
        "print(\"\\n--- Main Effects of Grouped CKD Diagnosis Categories ---\")\n",
        "diagnosis_main_effects = [col for col in loglogistic_summary_df.index.get_level_values(1) if col.startswith('grouped_diagnosis_') and '_x_' not in col and ('alpha_', col) in loglogistic_summary_df.index]\n",
        "for effect in diagnosis_main_effects:\n",
        "    try:\n",
        "        # Corrected indexing\n",
        "        row = loglogistic_summary_df.loc[('alpha_', effect)]\n",
        "        exp_coef = row['exp(coef)']\n",
        "        p_value = row['p']\n",
        "        if p_value < 0.05:\n",
        "            interpretation = f\"Statistically significant. An {exp_coef:.2f}x {'longer' if exp_coef > 1 else 'shorter'} expected time to dialysis compared to baseline (or other categories in one-hot encoding without `drop_first`).\"\n",
        "        else:\n",
        "            interpretation = \"Not statistically significant.\"\n",
        "        print(f\"* {effect}: exp(coef) = {exp_coef:.2f}, p = {p_value:.3f}. {interpretation}\")\n",
        "    except KeyError:\n",
        "        print(f\"* {effect}: Not found in summary or potentially dropped.\")\n",
        "\n",
        "# Interpret interaction terms\n",
        "print(\"\\n--- Interaction Terms: CKD Diagnosis Groups x GRT Categories ---\")\n",
        "interaction_terms = [col for col in loglogistic_summary_df.index.get_level_values(1) if '_x_' in col and ('alpha_', col) in loglogistic_summary_df.index]\n",
        "for term in interaction_terms:\n",
        "    try:\n",
        "        # Corrected indexing\n",
        "        row = loglogistic_summary_df.loc[('alpha_', term)]\n",
        "        exp_coef = row['exp(coef)']\n",
        "        p_value = row['p']\n",
        "        if p_value < 0.05:\n",
        "            interpretation = f\"Statistically significant. Indicates that the effect of the GRT category on time to dialysis significantly changes within this specific CKD diagnosis group. Acceleration factor = {exp_coef:.2f}.\"\n",
        "        else:\n",
        "            interpretation = \"Not statistically significant.\"\n",
        "        print(f\"* {term}: exp(coef) = {exp_coef:.2f}, p = {p_value:.3f}. {interpretation}\")\n",
        "    except KeyError:\n",
        "        print(f\"* {term}: Not found in summary or potentially dropped.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optuna Hyperparameter\n",
        "\n",
        "Install the Optuna library, then define and run Optuna studies for hyperparameter optimization of the comprehensive and main-effects penalized Cox Proportional Hazards and the 3 comprehensive parametric models. Subsequently, apply the best hyperparameters to re-train these models. Also utilize xgboost cox model to get a better model with more predictors that are statistically significant"
      ],
      "metadata": {
        "id": "nKMkOAS9X7dU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46fce516"
      },
      "source": [
        "## Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3d30837"
      },
      "source": [
        "!pip install optuna\n",
        "!pip install xgboost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc96a015"
      },
      "source": [
        "## Define Objective Functions for Optuna Studies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e25386b3"
      },
      "source": [
        "from lifelines import CoxPHFitter, WeibullAFTFitter, LogLogisticAFTFitter, LogNormalAFTFitter\n",
        "\n",
        "print(\"Lifelines Fitter classes imported.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74a8a0ae"
      },
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "\n",
        "# Objective function for Comprehensive Penalized CoxPH\n",
        "def objective_cox_comprehensive(trial):\n",
        "    penalizer = trial.suggest_float('penalizer', 1e-3, 1.0, log=True)\n",
        "    l1_ratio = trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
        "\n",
        "    cph = CoxPHFitter(penalizer=penalizer, l1_ratio=l1_ratio)\n",
        "    try:\n",
        "        cph.fit(cox_data_final, duration_col='time_to_event', event_col='event')\n",
        "        return cph.concordance_index_\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "# Objective function for Main-Effects Penalized CoxPH\n",
        "def objective_cox_main_effects(trial):\n",
        "    penalizer = trial.suggest_float('penalizer', 1e-3, 1.0, log=True)\n",
        "    l1_ratio = trial.suggest_float('l1_ratio', 0.0, 1.0)\n",
        "\n",
        "    cph = CoxPHFitter(penalizer=penalizer, l1_ratio=l1_ratio)\n",
        "    try:\n",
        "        cph.fit(cox_data_main_effects_cleaned, duration_col='time_to_event', event_col='event')\n",
        "        return cph.concordance_index_\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "# Objective function for Comprehensive Weibull AFT\n",
        "def objective_weibull_comprehensive(trial):\n",
        "    penalizer = trial.suggest_float('penalizer', 1e-3, 1.0, log=True)\n",
        "\n",
        "    weibull_aft = WeibullAFTFitter(penalizer=penalizer)\n",
        "    try:\n",
        "        weibull_aft.fit(comprehensive_aft_data.drop(columns=['time_to_event']), duration_col='time_to_event_scaled', event_col='event')\n",
        "        return weibull_aft.concordance_index_\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "# Objective function for Comprehensive Log-Logistic AFT\n",
        "def objective_loglogistic_comprehensive(trial):\n",
        "    penalizer = trial.suggest_float('penalizer', 1e-3, 1.0, log=True)\n",
        "\n",
        "    loglogistic_aft = LogLogisticAFTFitter(penalizer=penalizer)\n",
        "    try:\n",
        "        loglogistic_aft.fit(comprehensive_aft_data.drop(columns=['time_to_event']), duration_col='time_to_event_scaled', event_col='event')\n",
        "        return loglogistic_aft.concordance_index_\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "# Objective function for Comprehensive Log-Normal AFT\n",
        "def objective_lognormal_comprehensive(trial):\n",
        "    penalizer = trial.suggest_float('penalizer', 1e-3, 1.0, log=True)\n",
        "\n",
        "    lognormal_aft = LogNormalAFTFitter(penalizer=penalizer)\n",
        "    try:\n",
        "        lognormal_aft.fit(comprehensive_aft_data.drop(columns=['time_to_event']), duration_col='time_to_event_scaled', event_col='event')\n",
        "        return lognormal_aft.concordance_index_\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "print(\"Objective functions for Optuna defined.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecdcfb8d"
      },
      "source": [
        "## Run Optuna Studies for Each Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "338b7bf9"
      },
      "source": [
        "study_results = {}\n",
        "\n",
        "# 1. Comprehensive Penalized CoxPH\n",
        "print(\"\\n--- Running Optuna study for Comprehensive Penalized CoxPH ---\")\n",
        "study_cox_comprehensive = optuna.create_study(direction='maximize')\n",
        "study_cox_comprehensive.optimize(objective_cox_comprehensive, n_trials=50, show_progress_bar=True)\n",
        "study_results['cox_comprehensive'] = study_cox_comprehensive\n",
        "print(f\"Best trial for Comprehensive CoxPH (C-index): {study_cox_comprehensive.best_value:.4f}\")\n",
        "print(f\"Best hyperparameters for Comprehensive CoxPH: {study_cox_comprehensive.best_params}\")\n",
        "\n",
        "# 2. Main-Effects Penalized CoxPH\n",
        "print(\"\\n--- Running Optuna study for Main-Effects Penalized CoxPH ---\")\n",
        "study_cox_main_effects = optuna.create_study(direction='maximize')\n",
        "study_cox_main_effects.optimize(objective_cox_main_effects, n_trials=50, show_progress_bar=True)\n",
        "study_results['cox_main_effects'] = study_cox_main_effects\n",
        "print(f\"Best trial for Main-Effects CoxPH (C-index): {study_cox_main_effects.best_value:.4f}\")\n",
        "print(f\"Best hyperparameters for Main-Effects CoxPH: {study_cox_main_effects.best_params}\")\n",
        "\n",
        "# 3. Comprehensive Weibull AFT\n",
        "print(\"\\n--- Running Optuna study for Comprehensive Weibull AFT ---\")\n",
        "study_weibull_comprehensive = optuna.create_study(direction='maximize')\n",
        "study_weibull_comprehensive.optimize(objective_weibull_comprehensive, n_trials=50, show_progress_bar=True)\n",
        "study_results['weibull_comprehensive'] = study_weibull_comprehensive\n",
        "print(f\"Best trial for Comprehensive Weibull AFT (C-index): {study_weibull_comprehensive.best_value:.4f}\")\n",
        "print(f\"Best hyperparameters for Comprehensive Weibull AFT: {study_weibull_comprehensive.best_params}\")\n",
        "\n",
        "# 4. Comprehensive Log-Logistic AFT\n",
        "print(\"\\n--- Running Optuna study for Comprehensive Log-Logistic AFT ---\")\n",
        "study_loglogistic_comprehensive = optuna.create_study(direction='maximize')\n",
        "study_loglogistic_comprehensive.optimize(objective_loglogistic_comprehensive, n_trials=50, show_progress_bar=True)\n",
        "study_results['loglogistic_comprehensive'] = study_loglogistic_comprehensive\n",
        "print(f\"Best trial for Comprehensive Log-Logistic AFT (C-index): {study_loglogistic_comprehensive.best_value:.4f}\")\n",
        "print(f\"Best hyperparameters for Comprehensive Log-Logistic AFT: {study_loglogistic_comprehensive.best_params}\")\n",
        "\n",
        "# 5. Comprehensive Log-Normal AFT\n",
        "print(\"\\n--- Running Optuna study for Comprehensive Log-Normal AFT ---\")\n",
        "study_lognormal_comprehensive = optuna.create_study(direction='maximize')\n",
        "study_lognormal_comprehensive.optimize(objective_lognormal_comprehensive, n_trials=50, show_progress_bar=True)\n",
        "study_results['lognormal_comprehensive'] = study_lognormal_comprehensive\n",
        "print(f\"Best trial for Comprehensive Log-Normal AFT (C-index): {study_lognormal_comprehensive.best_value:.4f}\")\n",
        "print(f\"Best hyperparameters for Comprehensive Log-Normal AFT: {study_lognormal_comprehensive.best_params}\")\n",
        "\n",
        "print(\"\\nOptuna studies completed for all models.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9147a1a7"
      },
      "source": [
        "# Best hyperparameters from Optuna studies\n",
        "best_params_cox_comprehensive = study_results['cox_comprehensive'].best_params\n",
        "best_params_cox_main_effects = study_results['cox_main_effects'].best_params\n",
        "best_params_weibull_comprehensive = study_results['weibull_comprehensive'].best_params\n",
        "best_params_loglogistic_comprehensive = study_results['loglogistic_comprehensive'].best_params\n",
        "best_params_lognormal_comprehensive = study_results['lognormal_comprehensive'].best_params\n",
        "\n",
        "# --- Retrain Comprehensive Penalized CoxPH Model --- #\n",
        "print(\"\\n--- Retraining Comprehensive Penalized CoxPH Model with Optimized Hyperparameters ---\")\n",
        "cph_comprehensive_optimized = CoxPHFitter(penalizer=best_params_cox_comprehensive['penalizer'],\n",
        "                                          l1_ratio=best_params_cox_comprehensive['l1_ratio'])\n",
        "try:\n",
        "    cph_comprehensive_optimized.fit(cox_data_final, duration_col='time_to_event', event_col='event')\n",
        "    print(\"Comprehensive Penalized CoxPH Model Summary (Optimized):\")\n",
        "    cph_comprehensive_optimized.print_summary()\n",
        "    print(f\"Concordance Index: {cph_comprehensive_optimized.concordance_index_:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error retraining Comprehensive CoxPH: {e}\")\n",
        "\n",
        "# --- Retrain Main-Effects Penalized CoxPH Model --- #\n",
        "print(\"\\n--- Retraining Main-Effects Penalized CoxPH Model with Optimized Hyperparameters ---\")\n",
        "cph_main_effects_optimized = CoxPHFitter(penalizer=best_params_cox_main_effects['penalizer'],\n",
        "                                         l1_ratio=best_params_cox_main_effects['l1_ratio'])\n",
        "try:\n",
        "    cph_main_effects_optimized.fit(cox_data_main_effects_cleaned, duration_col='time_to_event', event_col='event')\n",
        "    print(\"Main-Effects Penalized CoxPH Model Summary (Optimized):\")\n",
        "    cph_main_effects_optimized.print_summary()\n",
        "    print(f\"Concordance Index: {cph_main_effects_optimized.concordance_index_:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error retraining Main-Effects CoxPH: {e}\")\n",
        "\n",
        "# --- Retrain Comprehensive Weibull AFT Model --- #\n",
        "print(\"\\n--- Retraining Comprehensive Weibull AFT Model with Optimized Hyperparameters ---\")\n",
        "weibull_aft_comprehensive_optimized = WeibullAFTFitter(penalizer=best_params_weibull_comprehensive['penalizer'])\n",
        "try:\n",
        "    weibull_aft_comprehensive_optimized.fit(comprehensive_aft_data.drop(columns=['time_to_event']),\n",
        "                                            duration_col='time_to_event_scaled',\n",
        "                                            event_col='event')\n",
        "    print(\"Weibull AFT Model Summary (Optimized):\")\n",
        "    weibull_aft_comprehensive_optimized.print_summary()\n",
        "    print(f\"Concordance Index: {weibull_aft_comprehensive_optimized.concordance_index_:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error retraining Comprehensive Weibull AFT: {e}\")\n",
        "\n",
        "# --- Retrain Comprehensive Log-Logistic AFT Model --- #\n",
        "print(\"\\n--- Retraining Comprehensive Log-Logistic AFT Model with Optimized Hyperparameters ---\")\n",
        "loglogistic_aft_comprehensive_optimized = LogLogisticAFTFitter(penalizer=best_params_loglogistic_comprehensive['penalizer'])\n",
        "try:\n",
        "    loglogistic_aft_comprehensive_optimized.fit(comprehensive_aft_data.drop(columns=['time_to_event']),\n",
        "                                                duration_col='time_to_event_scaled',\n",
        "                                                event_col='event')\n",
        "    print(\"Log-Logistic AFT Model Summary (Optimized):\")\n",
        "    loglogistic_aft_comprehensive_optimized.print_summary()\n",
        "    print(f\"Concordance Index: {loglogistic_aft_comprehensive_optimized.concordance_index_:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error retraining Comprehensive Log-Logistic AFT: {e}\")\n",
        "\n",
        "# --- Retrain Comprehensive Log-Normal AFT Model --- #\n",
        "print(\"\\n--- Retraining Comprehensive Log-Normal AFT Model with Optimized Hyperparameters ---\")\n",
        "lognormal_aft_comprehensive_optimized = LogNormalAFTFitter(penalizer=best_params_lognormal_comprehensive['penalizer'])\n",
        "try:\n",
        "    lognormal_aft_comprehensive_optimized.fit(comprehensive_aft_data.drop(columns=['time_to_event']),\n",
        "                                              duration_col='time_to_event_scaled',\n",
        "                                              event_col='event')\n",
        "    print(\"Log-Normal AFT Model Summary (Optimized):\")\n",
        "    lognormal_aft_comprehensive_optimized.print_summary()\n",
        "    print(f\"Concordance Index: {lognormal_aft_comprehensive_optimized.concordance_index_:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error retraining Comprehensive Log-Normal AFT: {e}\")\n",
        "\n",
        "print(\"All models retrained with optimized hyperparameters.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bb14a13"
      },
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Prepare data for XGBoost Cox model\n",
        "X_xgb = cox_data_final.drop(columns=['time_to_event', 'event'])\n",
        "\n",
        "# Create a structured array for the target variable (duration and event)\n",
        "y_xgb = np.array(list(zip(cox_data_final['time_to_event'], cox_data_final['event'])),\n",
        "                   dtype=[('f0', '<f8'), ('f1', '?')])\n",
        "\n",
        "print(\"Features (X_xgb) head:\")\n",
        "display(X_xgb.head())\n",
        "\n",
        "print(\"\\nTarget (y_xgb) first 5 rows:\")\n",
        "print(y_xgb[:5])\n",
        "\n",
        "print(\"\\nShape of X_xgb:\", X_xgb.shape)\n",
        "print(\"Shape of y_xgb:\", y_xgb.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09f68ae8"
      },
      "source": [
        "import optuna\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import KFold\n",
        "from lifelines.utils import concordance_index\n",
        "\n",
        "# Objective function for XGBoost Cox model\n",
        "def objective_xgboost_cox(trial):\n",
        "    params = {\n",
        "        'objective': 'survival:cox',\n",
        "        'eval_metric': 'cox-nloglik',\n",
        "        'eta': trial.suggest_float('eta', 1e-3, 0.1, log=True),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'lambda': trial.suggest_float('lambda', 1e-2, 1.0, log=True),\n",
        "        'alpha': trial.suggest_float('alpha', 1e-2, 1.0, log=True),\n",
        "        'tree_method': 'hist',\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "\n",
        "    n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
        "\n",
        "    # Use K-Fold cross-validation to get a robust C-index\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    c_indices = []\n",
        "\n",
        "    for train_index, test_index in kf.split(X_xgb):\n",
        "        X_train, X_test = X_xgb.iloc[train_index], X_xgb.iloc[test_index]\n",
        "        y_train, y_test = y_xgb[train_index], y_xgb[test_index]\n",
        "\n",
        "        # Prepare DMatrix for XGBoost with correct label encoding for survival:cox.\n",
        "        dtrain = xgb.DMatrix(X_train, label=np.where(y_train['f1'] == 1, y_train['f0'], -y_train['f0']))\n",
        "        dtest = xgb.DMatrix(X_test)\n",
        "\n",
        "        try:\n",
        "            model = xgb.train(params, dtrain, num_boost_round=n_estimators, verbose_eval=False)\n",
        "\n",
        "            # Predict risk scores on the test set\n",
        "            test_risk_scores = model.predict(dtest)\n",
        "\n",
        "            # Calculate C-index using lifelines utility\n",
        "            c_index = concordance_index(y_test['f0'], -test_risk_scores, y_test['f1'])\n",
        "            c_indices.append(c_index)\n",
        "        except Exception:\n",
        "            # Handle convergence issues or errors during fitting by returning a very low C-index\n",
        "            return 0.0\n",
        "\n",
        "    return np.mean(c_indices)\n",
        "\n",
        "print(\"Objective function for XGBoost Cox model defined.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec0f99cd"
      },
      "source": [
        "print(\"\\n--- Running Optuna study for XGBoost Cox Model (Corrected) ---\")\n",
        "study_xgboost_cox = optuna.create_study(direction='maximize')\n",
        "study_xgboost_cox.optimize(objective_xgboost_cox, n_trials=50, show_progress_bar=True)\n",
        "study_results['xgboost_cox'] = study_xgboost_cox\n",
        "print(f\"Best trial for XGBoost Cox Model (C-index): {study_xgboost_cox.best_value:.4f}\")\n",
        "print(f\"Best hyperparameters for XGBoost Cox Model: {study_xgboost_cox.best_params}\")\n",
        "\n",
        "print(\"Optuna study for XGBoost Cox model completed (Corrected).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5b7ca1e"
      },
      "source": [
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from lifelines.utils import concordance_index\n",
        "\n",
        "# Best hyperparameters from Optuna study for XGBoost Cox Model\n",
        "best_params_xgboost_cox = study_results['xgboost_cox'].best_params\n",
        "\n",
        "# --- Retrain XGBoost Cox Model with Optimized Hyperparameters ---\n",
        "print(\"\\n--- Retraining XGBoost Cox Model with Optimized Hyperparameters ---\")\n",
        "\n",
        "xgb_params_optimized = {\n",
        "    'objective': 'survival:cox',\n",
        "    'eval_metric': 'cox-nloglik',\n",
        "    'eta': best_params_xgboost_cox['eta'],\n",
        "    'max_depth': best_params_xgboost_cox['max_depth'],\n",
        "    'subsample': best_params_xgboost_cox['subsample'],\n",
        "    'colsample_bytree': best_params_xgboost_cox['colsample_bytree'],\n",
        "    'lambda': best_params_xgboost_cox['lambda'],\n",
        "    'alpha': best_params_xgboost_cox['alpha'],\n",
        "    'tree_method': 'hist',\n",
        "    'n_jobs': -1\n",
        "}\n",
        "\n",
        "n_estimators_optimized = best_params_xgboost_cox['n_estimators']\n",
        "\n",
        "# Prepare final DMatrix using the full dataset for retraining\n",
        "dmatrix_full = xgb.DMatrix(X_xgb, label=np.where(y_xgb['f1'] == 1, y_xgb['f0'], -y_xgb['f0']))\n",
        "\n",
        "# Train the final XGBoost Cox model\n",
        "xgboost_cox_optimized = xgb.train(xgb_params_optimized, dmatrix_full, num_boost_round=n_estimators_optimized, verbose_eval=False)\n",
        "\n",
        "# Predict risk scores on the full dataset\n",
        "final_risk_scores_xgb = xgboost_cox_optimized.predict(xgb.DMatrix(X_xgb))\n",
        "\n",
        "# Calculate C-index using lifelines utility, inverting risk scores as determined in Optuna\n",
        "c_index_xgboost_optimized = concordance_index(y_xgb['f0'], -final_risk_scores_xgb, y_xgb['f1'])\n",
        "\n",
        "print(f\"XGBoost Cox Model (Optimized) - Concordance Index: {c_index_xgboost_optimized:.4f}\")\n",
        "\n",
        "# Store the optimized model and its C-index for summary\n",
        "optimized_models = {\n",
        "    'Comprehensive Penalized CoxPH': {'model': cph_comprehensive_optimized, 'c_index': cph_comprehensive_optimized.concordance_index_},\n",
        "    'Main-Effects Penalized CoxPH': {'model': cph_main_effects_optimized, 'c_index': cph_main_effects_optimized.concordance_index_},\n",
        "    'Comprehensive Weibull AFT': {'model': weibull_aft_comprehensive_optimized, 'c_index': weibull_aft_comprehensive_optimized.concordance_index_},\n",
        "    'Comprehensive Log-Logistic AFT': {'model': loglogistic_aft_comprehensive_optimized, 'c_index': loglogistic_aft_comprehensive_optimized.concordance_index_},\n",
        "    'Comprehensive Log-Normal AFT': {'model': lognormal_aft_comprehensive_optimized, 'c_index': lognormal_aft_comprehensive_optimized.concordance_index_},\n",
        "    'XGBoost Cox Model': {'model': xgboost_cox_optimized, 'c_index': c_index_xgboost_optimized}\n",
        "}\n",
        "\n",
        "print(\"\\n--- Comparative Summary of Optimized Models (C-index) ---\")\n",
        "performance_summary = pd.DataFrame({\n",
        "    'Model': [name for name in optimized_models.keys()],\n",
        "    'C-index': [model_info['c_index'] for model_info in optimized_models.values()]\n",
        "})\n",
        "performance_summary = performance_summary.sort_values(by='C-index', ascending=False).reset_index(drop=True)\n",
        "display(performance_summary)\n",
        "\n",
        "# Identify the best performing model(s)\n",
        "best_model_name = performance_summary.loc[0, 'Model']\n",
        "best_model_c_index = performance_summary.loc[0, 'C-index']\n",
        "\n",
        "print(f\"\\nThe best performing model is the {best_model_name} with a C-index of {best_model_c_index:.4f}.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d795fc18"
      },
      "source": [
        "print(\"\\n--- Interpretation of Comprehensive Penalized CoxPH Model (Optimized) ---\")\n",
        "\n",
        "# Access the summary DataFrame of the optimized Comprehensive Penalized CoxPH model\n",
        "summary_cph_comprehensive_optimized = cph_comprehensive_optimized.summary.copy()\n",
        "\n",
        "# Remove the overall test rows for individual predictor interpretation\n",
        "rows_to_remove = ['log-likelihood ratio test', 'Concordance', 'Partial AIC', 'log-likelihood ratio test (p=?)']\n",
        "summary_cph_comprehensive_optimized = summary_cph_comprehensive_optimized[~summary_cph_comprehensive_optimized.index.isin(rows_to_remove)]\n",
        "\n",
        "alpha_level = 0.05\n",
        "\n",
        "print(\"Hazard ratios exp(coef) less than 1 indicate a decreased hazard (lower risk of dialysis).\")\n",
        "print(\"Hazard ratios exp(coef) greater than 1 indicate an increased hazard (higher risk of dialysis).\")\n",
        "print(\"Statistically significant predictors are those with p < 0.05.\")\n",
        "\n",
        "for index, row in summary_cph_comprehensive_optimized.iterrows():\n",
        "    if 'exp(coef)' in row and 'p' in row and 'exp(coef) lower 95%' in row and 'exp(coef) upper 95%' in row:\n",
        "        covariate = index\n",
        "        hazard_ratio = row['exp(coef)']\n",
        "        p_value = row['p']\n",
        "        ci_lower = row['exp(coef) lower 95%']\n",
        "        ci_upper = row['exp(coef) upper 95%']\n",
        "\n",
        "        if p_value < alpha_level:\n",
        "            significance = \"Statistically significant (p < 0.05)\"\n",
        "            if hazard_ratio < 1:\n",
        "                effect = \"decreased\"\n",
        "                strength = f\"protective effect against dialysis with a hazard ratio of {hazard_ratio:.2f}\"\n",
        "            else:\n",
        "                effect = \"increased\"\n",
        "                strength = f\"risk of dialysis with a hazard ratio of {hazard_ratio:.2f}\"\n",
        "            print(f\"* {covariate}: {significance}. Associated with {effect} {strength} (95% CI: {ci_lower:.2f} - {ci_upper:.2f}).\")\n",
        "    else:\n",
        "        print(f\"* Warning: Missing expected columns in summary row for {index}. Skipping interpretation.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "439be370"
      },
      "source": [
        "# SHAP values for XGBoost model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "659af786"
      },
      "source": [
        "!pip install shap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f78c9a5"
      },
      "source": [
        "print(\"Head of X_xgb DataFrame:\")\n",
        "display(X_xgb.head())\n",
        "\n",
        "print(\"\\nInfo of X_xgb DataFrame:\")\n",
        "X_xgb.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6200f369"
      },
      "source": [
        "import shap\n",
        "\n",
        "# 1. Initialize a SHAP TreeExplainer object\n",
        "explainer = shap.TreeExplainer(xgboost_cox_optimized, model_output='raw')\n",
        "\n",
        "# 2. Compute the SHAP values for the X_xgb dataset\n",
        "shap_values = explainer.shap_values(X_xgb)\n",
        "\n",
        "print(\"SHAP values computed successfully.\")\n",
        "print(f\"Shape of SHAP values: {shap_values.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d57772c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a SHAP summary plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "shap.summary_plot(shap_values, X_xgb, plot_type='beeswarm', show=False)\n",
        "plt.title('SHAP Summary Plot for XGBoost Cox Model')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Calculate mean absolute SHAP values for each feature\n",
        "# shap_values contains the SHAP values for each instance and each feature\n",
        "# X_xgb contains the feature names\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X_xgb.columns,\n",
        "    'Mean_Absolute_SHAP_Value': np.abs(shap_values).mean(axis=0)\n",
        "})\n",
        "\n",
        "# Sort by Mean_Absolute_SHAP_Value in descending order\n",
        "feature_importance = feature_importance.sort_values(by='Mean_Absolute_SHAP_Value', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nSHAP Feature Importance Table (Top 20 Most Important Features):\")\n",
        "display(feature_importance.head(20))\n",
        "\n",
        "# You can adjust .head(20) to display more or fewer features."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "7t_vlf82rq0w",
        "outputId": "d94dc536-a1b0-48eb-8c69-10afff71b9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SHAP Feature Importance Table (Top 20 Most Important Features):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                              Feature  \\\n",
              "0   grouped_diagnosis_Hypertensive CKD (Stages 5-E...   \n",
              "1                              num_ckd_grt_categories   \n",
              "2                                             on_arbs   \n",
              "3   grouped_diagnosis_Hypertensive CKD (Stages 5-E...   \n",
              "4                       grouped_diagnosis_Stage 5 CKD   \n",
              "5                       grouped_diagnosis_Stage 4 CKD   \n",
              "6                                    on_ace_inhibitor   \n",
              "7                                           on_statin   \n",
              "8                       grouped_diagnosis_Stage 3 CKD   \n",
              "9           grouped_diagnosis_Stage 3 CKD_x_on_statin   \n",
              "10   grouped_diagnosis_Stage 4 CKD_x_on_ace_inhibitor   \n",
              "11    grouped_diagnosis_Hypertensive CKD (Stages 1-4)   \n",
              "12  grouped_diagnosis_Hypertensive CKD (Stages 5-E...   \n",
              "13          grouped_diagnosis_Stage 5 CKD_x_on_statin   \n",
              "14   grouped_diagnosis_Stage 5 CKD_x_on_ace_inhibitor   \n",
              "15  grouped_diagnosis_Hypertensive CKD (Stages 1-4...   \n",
              "16                                 on_sglt2_inhibitor   \n",
              "17                  grouped_diagnosis_Unspecified CKD   \n",
              "18  grouped_diagnosis_Hypertensive CKD (Stages 5-E...   \n",
              "19          grouped_diagnosis_Stage 4 CKD_x_on_statin   \n",
              "\n",
              "    Mean_Absolute_SHAP_Value  \n",
              "0                   0.370502  \n",
              "1                   0.151876  \n",
              "2                   0.138018  \n",
              "3                   0.078273  \n",
              "4                   0.066077  \n",
              "5                   0.057940  \n",
              "6                   0.057384  \n",
              "7                   0.042422  \n",
              "8                   0.034212  \n",
              "9                   0.025393  \n",
              "10                  0.017447  \n",
              "11                  0.017251  \n",
              "12                  0.014657  \n",
              "13                  0.011379  \n",
              "14                  0.005762  \n",
              "15                  0.004876  \n",
              "16                  0.004821  \n",
              "17                  0.004709  \n",
              "18                  0.003462  \n",
              "19                  0.003094  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a5dc5d95-ff6f-4378-81ed-8ca494fbe721\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature</th>\n",
              "      <th>Mean_Absolute_SHAP_Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>grouped_diagnosis_Hypertensive CKD (Stages 5-E...</td>\n",
              "      <td>0.370502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>num_ckd_grt_categories</td>\n",
              "      <td>0.151876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>on_arbs</td>\n",
              "      <td>0.138018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>grouped_diagnosis_Hypertensive CKD (Stages 5-E...</td>\n",
              "      <td>0.078273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>grouped_diagnosis_Stage 5 CKD</td>\n",
              "      <td>0.066077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>grouped_diagnosis_Stage 4 CKD</td>\n",
              "      <td>0.057940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>on_ace_inhibitor</td>\n",
              "      <td>0.057384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>on_statin</td>\n",
              "      <td>0.042422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>grouped_diagnosis_Stage 3 CKD</td>\n",
              "      <td>0.034212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>grouped_diagnosis_Stage 3 CKD_x_on_statin</td>\n",
              "      <td>0.025393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>grouped_diagnosis_Stage 4 CKD_x_on_ace_inhibitor</td>\n",
              "      <td>0.017447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>grouped_diagnosis_Hypertensive CKD (Stages 1-4)</td>\n",
              "      <td>0.017251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>grouped_diagnosis_Hypertensive CKD (Stages 5-E...</td>\n",
              "      <td>0.014657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>grouped_diagnosis_Stage 5 CKD_x_on_statin</td>\n",
              "      <td>0.011379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>grouped_diagnosis_Stage 5 CKD_x_on_ace_inhibitor</td>\n",
              "      <td>0.005762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>grouped_diagnosis_Hypertensive CKD (Stages 1-4...</td>\n",
              "      <td>0.004876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>on_sglt2_inhibitor</td>\n",
              "      <td>0.004821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>grouped_diagnosis_Unspecified CKD</td>\n",
              "      <td>0.004709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>grouped_diagnosis_Hypertensive CKD (Stages 5-E...</td>\n",
              "      <td>0.003462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>grouped_diagnosis_Stage 4 CKD_x_on_statin</td>\n",
              "      <td>0.003094</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a5dc5d95-ff6f-4378-81ed-8ca494fbe721')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a5dc5d95-ff6f-4378-81ed-8ca494fbe721 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a5dc5d95-ff6f-4378-81ed-8ca494fbe721');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-06244372-6289-4216-8fdc-e7d91cc9d5f8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-06244372-6289-4216-8fdc-e7d91cc9d5f8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-06244372-6289-4216-8fdc-e7d91cc9d5f8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"# You can adjust \",\n  \"rows\": 20,\n  \"fields\": [\n    {\n      \"column\": \"Feature\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"grouped_diagnosis_Hypertensive CKD (Stages 5-End Stage)\",\n          \"grouped_diagnosis_Unspecified CKD\",\n          \"grouped_diagnosis_Hypertensive CKD (Stages 1-4)_x_on_statin\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Mean_Absolute_SHAP_Value\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.3705023229122162,\n          0.004709482658654451,\n          0.004875974729657173\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}